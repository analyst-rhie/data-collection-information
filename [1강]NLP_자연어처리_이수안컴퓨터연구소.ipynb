{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[1강]NLP_자연어처리_이수안컴퓨터연구소.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN6A0C8rGJPTj/WXX4z9Qy8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/analyst-rhie/data-collection-information/blob/main/%5B1%EA%B0%95%5DNLP_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%EC%9D%B4%EC%88%98%EC%95%88%EC%BB%B4%ED%93%A8%ED%84%B0%EC%97%B0%EA%B5%AC%EC%86%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chRcS5jBolGv"
      },
      "source": [
        "## NLP(Natural Language Processing) 살펴보기\n",
        "\n",
        "## 본 코드는 이수안컴퓨터연구소 강의를 보며 정리한 강의노트입니다.\n",
        "https://www.youtube.com/watch?v=2e9wnwuAVv0&list=PL7ZVZgsnLwEEoHQAElEPg7l7T6nt25I3N\n",
        "\n",
        "### 자연어란  무엇인가?\n",
        "\n",
        "- 자신을 표현하는데 사용하는 언어인 기본적인 의사소통 수단이다.\n",
        "    \n",
        "    언어란 서로 의사소통하기 위해 사용하는 단어/소리를 포함하는 상호 합의된 프로토콜 집합\n",
        "    \n",
        "- 디지털화와 연산의 시대에는 언어를 과학적으로 이해하는 경향이 있다.\n",
        "    \n",
        "    무생물이 우리를 이해하도록 끊임없이 노력해 가고 있음. 따라서 같은 무생물에 언어를 제공하는 메커니즘을 개발하는 것이 필수적 이며 이를 NLP가 도와줄 수 있다.\n",
        "    \n",
        "    → 예) 사서함에 스펨 레이블이 자동으로 지정돼 이메일들을 처리한다. 이는 NLP의 도움으로 이뤄졌다. 여기서 이메일 서비스인 무생물 개체는 이메일의 내용을 1)분석해 2)이해한 후 이러한 이3)메일을 스팸으로 표시해야 하는지 여부를 결정한다.\n",
        "\n",
        "### 텍스트 분석과 NLP\n",
        "\n",
        "- 텍스트 분석은 의미 있는 인사이트를 추출하고 텍스트 데이터에서 질문에 대답하는 방법이다. 텍스트 데이터가 꼭 사람의 언어일 필요는 없다.\n",
        "    \n",
        "    ex) 발신전화 밎 SNS 로그 데이터가 그림 1.1와 같은 형식으로 포함된 텍스트 파일이라 가정하자.\n",
        "    \n",
        "    ![https://raw.githubusercontent.com/TrainingByPackt/Natural-Language-Processing-Fundamentals/master/Graphics/Lesson%2001/Figure%201.01.png](https://raw.githubusercontent.com/TrainingByPackt/Natural-Language-Processing-Fundamentals/master/Graphics/Lesson%2001/Figure%201.01.png)\n",
        "    \n",
        "    ![https://raw.githubusercontent.com/TrainingByPackt/Natural-Language-Processing-Fundamentals/master/Graphics/Lesson%2001/Figure%201.02.png](https://raw.githubusercontent.com/TrainingByPackt/Natural-Language-Processing-Fundamentals/master/Graphics/Lesson%2001/Figure%201.02.png)\n",
        "    \n",
        "    본 그림의 데이터는 정확히 사람의 언어가 아니지만 분석을 통해 추출할 수 있는 다양한 정보가 포함된다. 이 데이터를 보고 대답할 수 있는 질문은 다음과 같다.\n",
        "    \n",
        "    - 1월1일에 SMS로 몇 가지 새해 인사를 보냈는가?\n",
        "    - 연락처 목록에 이름이 없는 사람은 몇 명인가?\n",
        "    \n",
        "    이렇듯 주어진 텍스트 데이터에서 유용한 인사이트를 추출하는 기술을 **텍스트 분석**이라 한다. \n",
        "    \n",
        "    ### NLP와 텍스트 분석의 가장 큰 차이는 ?\n",
        "    \n",
        "    - NLP는 텍스트 데이터를 포괄하여 음성(voice)인식과 분석도 NLP의 영역에 속한다는 점이다.\n",
        "\n",
        "### NLP는 크게 두 종류로 나뉜다.\n",
        "\n",
        "- **NLU** : 연산 능력이 있는 무생물이 사람이 말하는 언어를 이해할 수 있는 언어로 바꾸는 프로세스를 의미한다. \n",
        "(사람의 언어→ 무생물의 언어)\n",
        "\n",
        "- **NLG** : 연산 능력이 있는 무생물 개체가 사람이 이해할 수 있는 언어로 생각을 표현할 수 있는 프로세스를 의미한다. \n",
        "(무생물의 언어→ 사람의 언어)\n",
        "\n",
        "- 예시 : 사람이 기계(연산 능력이 있는 무생물)와 대화할 때 \n",
        "1. 기계는 NLU 프로세스의 도움으로 사람의 언어를 해석\n",
        "2. NLG 프로세스를 사용해 기계는 적절한 응답을 생성해 인간과 공유"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmlZcpBBonDf"
      },
      "source": [
        "# 기본적인 텍스트 분석\n",
        "s = 'No pain no gain'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C74S021opZ1D",
        "outputId": "78a319bc-6077-40cc-b34b-e44809feaecb"
      },
      "source": [
        "'pain' in s"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wquph-FdsTy6",
        "outputId": "3ea35eb8-db98-4775-aa8e-a992d6241ed8"
      },
      "source": [
        "s.split()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['No', 'pain', 'no', 'gain']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJJ3O7QTsVyU",
        "outputId": "fa114319-afc8-427f-f807-87f91a80582a"
      },
      "source": [
        "s.split().index('gain') #어디 위치해 있는지 질의"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-5ocuvfIsUtR",
        "outputId": "615192cf-0e21-40dd-8c0c-a1bdf8f9db6f"
      },
      "source": [
        "s[-4:] #뒤에서 부터 출력"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gain'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VmJ9-VYJsgxq",
        "outputId": "ed958e8f-e12f-4093-dacf-b87c2b026822"
      },
      "source": [
        "s.split()[1]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'pain'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8JynzD08skat",
        "outputId": "39f81442-c33c-4ce8-d0cf-f3344ac7c23b"
      },
      "source": [
        "s.split()[2][::-1] #[2]까지하면 no가 뜨는데 [::-1]을 주면 뒤집혀서 출력됨"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'on'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWc3OwAnsppa"
      },
      "source": [
        "s = \"한글도 처리 가능\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2xyUOCoszci",
        "outputId": "a3cedab5-0086-4a8a-86b3-2a9da68760e5"
      },
      "source": [
        "\"처리\" in s"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ey5Fd9gs1as",
        "outputId": "2508d027-4f14-459c-f79b-d04cd0e587b0"
      },
      "source": [
        "s.split()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['한글도', '처리', '가능']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0n3bQkems29b",
        "outputId": "aa97d717-9dd3-4acd-dfef-048e2b643a9e"
      },
      "source": [
        "s.split()[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'한글도'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQMVXX57tAH-"
      },
      "source": [
        "## 영어 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFFrHFR9tC6U"
      },
      "source": [
        "* 같은 언어지만 한글과 영어는 굉장히 다르다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeunRpUPtIVe"
      },
      "source": [
        "### 대소문자 통합\n",
        "* 대소문자를 통합하지 않는다면 컴퓨터는 같은 단어를 다르게 받아들임\n",
        "* 파이썬의 내장함수 lower(), upper()를 통해 간단하게 통합가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNDTkczYs4oG",
        "outputId": "68c210b7-0227-4179-83b1-162834ac78ed"
      },
      "source": [
        "s = 'AbCdEfGh'\n",
        "str_lower = s.lower()\n",
        "str_upper = s.upper()\n",
        "print(str_lower, str_upper)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abcdefgh ABCDEFGH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji0jf3SOtlO9"
      },
      "source": [
        "### 정규화(Normalization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAVRK3KktoKo",
        "outputId": "94f8c8b2-67c6-4c8b-a60b-8f52150c13a5"
      },
      "source": [
        "s = \"I visited UK from US on 22-09-20\"\n",
        "print(s)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I visited UK from US on 22-09-20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T999YmcVtwey"
      },
      "source": [
        "* US, UK 또는 날짜는 통일된 명칭으로 통일할 필요가 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHsJx_5ps9LL",
        "outputId": "dfe3a4c6-8b79-4c0c-e642-08ce5765ac64"
      },
      "source": [
        "new_s = s.replace(\"UK\",\"United Kingdom\").replace(\"US\",\"United States\").replace(\"-20\",\"-2020\")\n",
        "print(new_s)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I visited United Kingdom from United States on 22-09-2020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1SrwcBEuWmV"
      },
      "source": [
        "### 정규표현식\n",
        " * 위와 같이 여러 단어들을 하나하나 작성하지 않고 편리하게 작업을 도와주는 도구다.\n",
        " * 정규 표현식은 특정 문자들을 편리하게 지정하고 추가, 삭제 가능\n",
        " * 데이터 전처리에서 정규 표현식을 많이 사용\n",
        " * 파이썬에서는 정규 표현식을 지원하는 re 패키지 지원한다.\n",
        " * 정규 표현식 문법\n",
        "\n",
        "| 특수문자 | 설명 |\n",
        "| - | - |\n",
        "| `.` | 앞의 문자 1개를 표현 |\n",
        "| `?` | 문자 한개를 표현하나 존재할 수도, 존재하지 않을 수도 있음(0개 또는 1개) |\n",
        "| `*` | 앞의 문자가 0개 이상 |\n",
        "| `+` | 앞의 문자가 최소 1개 이상 |\n",
        "| `^` | 뒤의 문자로 문자열이 시작 |\n",
        "| `\\$` | 앞의 문자로 문자열이 끝남 |\n",
        "| `\\{n\\}` | `n`번만큼 반복 |\n",
        "| `\\{n1, n2\\}` | `n1` 이상, `n2` 이하만큼 반복, n2를 지정하지 않으면 `n1` 이상만 반복 |\n",
        "| `\\[ abc \\]` | 안에 문자들 중 한 개의 문자와 매치, a-z처럼 범위도 지정 가능 |\n",
        "| `\\[ ^a \\]` | 해당 문자를 제외하고 매치 |\n",
        "| `a\\|b` | `a` 또는 `b`를 나타냄 |\n",
        "\n",
        "* 정규 표현식에 자주 사용하는 역슬래시(\\)를 이용한 문자 규칙\n",
        "\n",
        "| 문자 | 설명 |\n",
        "| - | - |\n",
        "| `\\\\` | 역슬래시 자체를 의미 |\n",
        "| `\\d` | 모든 숫자를 의미, [0-9]와 동일 |\n",
        "| `\\D` | 숫자를 제외한 모든 문자를 의미, [^0-9]와 동일 |\n",
        "| `\\s` | 공백을 의미, [ \\t\\n\\r\\f\\v]와 동일|\n",
        "| `\\S` | 공백을 제외한 모든 문자를 의미, [^ \\t\\n\\r\\f\\v]와 동일 |\n",
        "| `\\w` | 문자와 숫자를 의미, [a-zA-Z0-9]와 동일 |\n",
        "| `\\W` | 문자와 숫자를 제외한 다른 문자를 의미, [^a-zA-Z0-9]와 동일 |  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvMxsTKWwvsW"
      },
      "source": [
        "### match\n",
        "* 컴파일한 정규 표현식을 이용해 문자열이 정규 표현식과 맞는지 검사"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjJnLXRVt9HS"
      },
      "source": [
        "import re"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qktDnCYxBip",
        "outputId": "48d45dc1-0334-4921-98f2-822a7a9b3a83"
      },
      "source": [
        "check = 'ab.' # .은 하나의 문자열이 반드시 옴을 의미. 기준이 됨.\n",
        "print(re.match(check, 'abc')) # ab뒤에 어떤 문자(c)가 나왔기 떄문에 완전 매치됨.\n",
        "print(re.match(check, 'c')) # ab와 맞지 않음\n",
        "print(re.match(check, 'ab')) # 뒤에 문자열이 오지 않아서 매치 안됨."
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 3), match='abc'>\n",
            "None\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC6BD7PKxfal"
      },
      "source": [
        "### compile\n",
        "* compile을 사용하면 여러 번 사용할 경우 일반 사용보다 더 빠른 속도를 보임\n",
        "* compile을 통해 정규 표현식을 사용할 경우 're'가 아닌 컴파일한 객체 이름을 통해 사용해야 함."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1H8t2ACxNnT",
        "outputId": "ebaabcc6-cde6-46e4-8f39-b2b97a554d95"
      },
      "source": [
        "import time\n",
        "normal_s_time = time.time()\n",
        "r = 'ab.'\n",
        "for i in range(1000):\n",
        "  re.match(check, 'abc')\n",
        "print('일반 사용시 소요 시간: ', time.time() - normal_s_time)\n",
        "\n",
        "compile_s_time = time.time()\n",
        "r = re.compile('ab.')\n",
        "for i in range(1000):\n",
        "  r.match(check)\n",
        "print('컴파일 사용시 소요 시간: ', time.time() - compile_s_time)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "일반 사용시 소요 시간:  0.0031757354736328125\n",
            "컴파일 사용시 소요 시간:  0.0007412433624267578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmFSs37AylD_"
      },
      "source": [
        "### search\n",
        "* match와 다르게, search는 문자열의 전체를 검사"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWNuRkneyfMt",
        "outputId": "6c505b80-752e-4fac-f35d-b33569d1aaf6"
      },
      "source": [
        "check='ab?' #?는 앞의 b가 올 수도있고, 안올수도 있다. 기준이 됨.\n",
        "print(re.search('a',check)) # a만 있어도 매칭이 됨.\n",
        "print(re.match('kkkab',check)) #\n",
        "print(re.search('kkkab',check)) # 전체 검사시 a앞에 k가 없어서 나가리\n",
        "print(re.search('ab',check)) # 조건에 만족하여 리턴"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 1), match='a'>\n",
            "None\n",
            "None\n",
            "<re.Match object; span=(0, 2), match='ab'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV_iWCwcyrax"
      },
      "source": [
        "### split\n",
        "* 정규표현식에 해당하는 문자열을 기준으로 문자열을 나눔"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjoTl3dvyqsW",
        "outputId": "88273ff2-26ce-4aaa-d855-34d7e85ff80a"
      },
      "source": [
        "r = re.compile(' ') #' _ ' 기준이 되어 잘림\n",
        "print(r.split('abc abbc abcbab'))\n",
        "r = re.compile('c') # ' c ' 기준이 되어 잘림. 띄어쓰기도 그대로 포함됨.\n",
        "print(r.split('abc abbc abcbab'))\n",
        "r = re.compile('[1-9]') # 숫자가 기준이 되어 잘림.띄어쓰기도 그대로 포함됨.\n",
        "print(r.split('s1abc 2v3s 4sss 5a'))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abc', 'abbc', 'abcbab']\n",
            "['ab', ' abb', ' ab', 'bab']\n",
            "['s', 'abc ', 'v', 's ', 'sss ', 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tXVK4nS5HUt"
      },
      "source": [
        "### sub\n",
        "* 정규 표현식과 일치하는 부분을 다른 문자열로 교체"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pR9uxUA15MnM",
        "outputId": "8aad6424-59a2-488e-835a-9d2ea51dbebe"
      },
      "source": [
        "print(re.sub('[a-z]','abcdefg','1')) #a-z의 일치하는 부분을 abcdefg에서 1로 바꿔라\n",
        "print(re.sub('[^a-z]', 'abc defg', '1')) # a-z에 일치하지 않은 부분이므로 abc defg 그대로 출력"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "abc defg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up-Z93d56VGw"
      },
      "source": [
        "### findall\n",
        "* 컴파일한 정규 표현식을 이용해 정규 표현식과 맞는 모든 문자(열)을 리스트로 반환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcpPSRtN5Sg0",
        "outputId": "13c0e8b6-2ad9-446f-edc4-81401ba38b02"
      },
      "source": [
        "print(re.findall('[\\d]','1ab 2cd 3ef 4g')) #d는 숫자만 찾아줘\n",
        "print(re.findall('[\\W]','!abcd@@#')) # W는 문자 숫자가아닌 특수 문자만 찾아줘"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1', '2', '3', '4']\n",
            "['!', '@', '@', '#']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUCEc43a65dd"
      },
      "source": [
        "### finditer\n",
        "* 컴파일한 정규 표현식을 이용해 정규 표현식과 맞는 모든 문자(열)을 리스트가 아닌iterator 객체로 반환\n",
        "* iterator 객체를 이용하면 생성된 객체를 하나씩 자동으로 가져올 수 있어 처리가 간편함."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzDCcShd6zw6",
        "outputId": "62f795ed-7a08-4d1e-a716-0221a4691d3a"
      },
      "source": [
        "iterl = re.finditer('[\\d]','1ab 2cd 3ef 4g')\n",
        "print(iterl)\n",
        "for i in iterl:\n",
        "  print(i)\n",
        "iter2 = re.finditer('[\\W]','!abcd@@#')\n",
        "print(iter2)\n",
        "for i in iter2:\n",
        "  print(i)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<callable_iterator object at 0x7f34dd00cc90>\n",
            "<re.Match object; span=(0, 1), match='1'>\n",
            "<re.Match object; span=(4, 5), match='2'>\n",
            "<re.Match object; span=(8, 9), match='3'>\n",
            "<re.Match object; span=(12, 13), match='4'>\n",
            "<callable_iterator object at 0x7f34dcfd3c50>\n",
            "<re.Match object; span=(0, 1), match='!'>\n",
            "<re.Match object; span=(5, 6), match='@'>\n",
            "<re.Match object; span=(6, 7), match='@'>\n",
            "<re.Match object; span=(7, 8), match='#'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brdGTetW7zvh"
      },
      "source": [
        "### 토큰화(Tokenization)\n",
        "* 자연어에서 토큰화는 매우 기본\n",
        "* 토큰화하면서 주의할 점!! 아래 두가지\n",
        "  * 특수 문자에 대한 처리\n",
        "    * 단어에 일반적으로 사용되는 알파벳, 숫자와는 다르게 특수문자는 별도의 처리가 필요\n",
        "    * 일괄적으로 단어의 특수문자를 제거하는 방법도 있지만 특수문자가 단어에 특별한 의미를 가질 떄 이를 학습에 반영시키지 못할 수도 있음.\n",
        "      * ex) 의문문 표현인 ?와 !를 어떻게 학습시킬까?\n",
        "    * 특수문자에 대한 일괄적인 제거보다는 데이터의 특성을 파악하고, 처리를 하는것이 중요\n",
        "  * 특정 단어에 대한 토큰 분리 방법\n",
        "    * 한 단어지만 토큰으로 분리할 때 판단되는 문자들로 이루어진 we're, United Kingdom 등의 단어는 어떻게 분리해야 할지 선택이 필요.\n",
        "    * we're은 단어로 분리해도되나 United Kindom은 두 단어가 모여 특정 의미를 가리켜 분리해선 안됨.\n",
        "    * 사용자가 단어의 특성을 고려해 토큰을 분리하는 것이 학습에 유리\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTezyHm39YDF"
      },
      "source": [
        "### 단어 토큰화\n",
        "  * 파이썬 내장 함수인 split을 활용해 단어 토큰화\n",
        "  * 공백을 기준으로 단어를 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1goWWSKk7nUw",
        "outputId": "ca2d5bf7-5b00-4931-ef06-6434aff6103d"
      },
      "source": [
        "sentence = 'Time is gold'\n",
        "tokens = [x for x in sentence.split(' ')] # sentence를 띄어쓰기로 나눠서 리스트의 요소로 저장\n",
        "tokens"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Time', 'is', 'gold']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkm9r_o19wMp"
      },
      "source": [
        "* 토큰화는 nltk 패키지의 tokenize 모듈을 사용해 손쉽게 구현 가능.\n",
        "* 단어 토큰화는 word_tokenize() 함수를 사용해 구현 가능\n",
        "* **일반적인 split쓰는 것보다 토큰화 처리에 유명한 nltk 패키지를 사용하는 것이 좋다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5YDeMGZ9sGQ",
        "outputId": "56fbe21b-d1a3-4001-818a-a169aacb3dfa"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aYL2zNf-C_x",
        "outputId": "527cce63-6d6e-419d-b8a6-f117496b18ee"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Time', 'is', 'gold']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEaVRx59-c0J"
      },
      "source": [
        "### 문장 토큰화\n",
        "* 문장 토큰화는 줄바꿈 문자 \\n 을 기준으로 문장을 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV52xoSt-KXA",
        "outputId": "a3f595b8-15d6-497e-a23c-52220697c4cb"
      },
      "source": [
        "sentences = 'The world is a beautiful book. \\nBut of little use to him who cannot read it'\n",
        "print(sentences)\n",
        "\n",
        "tokens = [x for x in sentences.split('\\n')] # 띄어쓰기 기준으로 나눔.\n",
        "tokens"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The world is a beautiful book. \n",
            "But of little use to him who cannot read it\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The world is a beautiful book. ',\n",
              " 'But of little use to him who cannot read it']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGNANJs2-3xW"
      },
      "source": [
        "* 문장 토큰화는 sent_tokenize() 함수를 사용해 구현 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO_lRMk6-xgP",
        "outputId": "521786e5-ae9b-4865-ccaf-dab6cd5227e9"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "tokens = sent_tokenize(sentences)\n",
        "tokens"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The world is a beautiful book.',\n",
              " 'But of little use to him who cannot read it']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAYf6Q7J_GNu"
      },
      "source": [
        "* 문장 토큰화에서는 온점(.)의 처리를 위해 이진 분류기를 사용할 수도 있음\n",
        "* 온점은 문장과 문장을 구분해줄 수도, 문장에 포함된 단어를 구성할 수도 있기 때문에 이를 이진 분류기로 분류해 더욱 좋은 토큰화를 구현할 수도 있음."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHgRRLQ0_TqH"
      },
      "source": [
        "### 정규 표현식을 이용한 토큰화\n",
        "* 토큰화 기능을 직접 구현할 수도 있지만 정규 표현식을 이용해 간단하게 구현할 수도 있음.\n",
        "* nltk 패키지는 정규 표현식을 사용하는 토큰화 도구인 RegexpTokenizer를 제공"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO32XYME_Cob",
        "outputId": "092ba8ac-1a1d-4a8b-877c-c9f3ad0187e8"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "sentence = 'Where there\\'s a will, there\\'s a way'\n",
        "tokenizer = RegexpTokenizer(\"[\\w]+\") # w는 문자와 숫자를 가져오겠다. +는 최소 1개이상, 즉 \n",
        "# 문자와 숫자가 최소 1개이상인것을 가져오겠다 란 의미\n",
        "token = tokenizer.tokenize(sentence)\n",
        "token"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Where', 'there', 's', 'a', 'will', 'there', 's', 'a', 'way']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV2qolixAHCs",
        "outputId": "50599be4-f423-4153-ce80-1a5747da2f71"
      },
      "source": [
        "tokenizer = RegexpTokenizer(\"[\\s]+\", gaps=True) #s 는 특수문자를 남기고 그 공백을 기준으로 토큰화함.\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "tokens\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Where', \"there's\", 'a', 'will,', \"there's\", 'a', 'way']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OSNneSvA6ih"
      },
      "source": [
        "### 케라스를 이용한 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAQ0rm3LAhzl"
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "sentence = 'Where there\\'s a waill, there\\'s a way'\n",
        "text_to_word_sequence(sentence) # 특수문자는 남기고 스페이스 기준으로 토큰화 됨."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvT-7uRDBP7T"
      },
      "source": [
        "### TextBlob 을 이용한 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYeMF-4EBSGf",
        "outputId": "e2dff28f-1c53-4cb2-bd47-9fb52bb6255b"
      },
      "source": [
        "from textblob import TextBlob\n",
        "sentence = 'Where there\\'s a will, there\\'s a way'\n",
        "blob = TextBlob(sentence) # 's 를 하나의 토큰으로 인식함.\n",
        "blob.words"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['Where', 'there', \"'s\", 'a', 'will', 'there', \"'s\", 'a', 'way'])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w95rvL5BkgQ"
      },
      "source": [
        "### 기타 토크나이저\n",
        "* 각 목적에 맞게 토큰화하여 사용하자.\n",
        "  * WhiteSpaceTokenizer : 공백을 기준으로 토큰화\n",
        "  * WordPunkTokenizer : 텍스트를 알파벳 문자, 숫자, 알파벳 이외의 문자 리스트로 토큰화\n",
        "  * MWETokenizer : MWE는 Multi-Word Expresion의 약자로 'republic of korea'와 같이 여러 단어로 이뤄진 특정 그룹을 개체로 취급\n",
        "  * TweetTokenizer : 트위터에서 사용되는 문장의 토큰화를 위해서 만들어졌으며, 문장 속 감성의 표현과 감정을 다룸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XliE4ieyCAmV"
      },
      "source": [
        "### n-gram 추출\n",
        "* n-gram은 n개의 어절이나 음절을 연쇄적으로 분류해 그 빈도를 분석\n",
        "* n=1일 떄는 unigram, n=2일 때는 bigram, n=3일 때는 trigram으로 불림"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGHTwC3JBZ5_",
        "outputId": "3fac2973-19c2-4c4c-98cc-25e4c93bff5e"
      },
      "source": [
        "from nltk import ngrams\n",
        "sentence = 'There is no royal road to learning'\n",
        "# 하나씩(unigram)만 하면 unigram으로 토큰화한 결과가 같음.\n",
        "bigram = list(ngrams(sentence.split(),2))\n",
        "# ngram의 n=2일 때이므로 bigram 이다.\n",
        "print(bigram)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('There', 'is'), ('is', 'no'), ('no', 'royal'), ('royal', 'road'), ('road', 'to'), ('to', 'learning')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3TMrSi3DOrx",
        "outputId": "40e95edc-29b6-4788-c131-ff79266b5586"
      },
      "source": [
        "trigram = list(ngrams(sentence.split(),3))\n",
        "print(trigram)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('There', 'is', 'no'), ('is', 'no', 'royal'), ('no', 'royal', 'road'), ('royal', 'road', 'to'), ('road', 'to', 'learning')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz9DJAu-DaiS",
        "outputId": "c46db930-c193-4228-d053-9ee65a8858c2"
      },
      "source": [
        "# 다른 패키지 내의 ngram\n",
        "from textblob import TextBlob\n",
        "blob = TextBlob(sentence)\n",
        "blob.ngrams(n=2) #디폴트는 n=3이다."
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['There', 'is']),\n",
              " WordList(['is', 'no']),\n",
              " WordList(['no', 'royal']),\n",
              " WordList(['royal', 'road']),\n",
              " WordList(['road', 'to']),\n",
              " WordList(['to', 'learning'])]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iwivQh5Ds58",
        "outputId": "9333024f-b42c-4dcf-c208-365c3f0026db"
      },
      "source": [
        "blob.ngrams(n=3)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['There', 'is', 'no']),\n",
              " WordList(['is', 'no', 'royal']),\n",
              " WordList(['no', 'royal', 'road']),\n",
              " WordList(['royal', 'road', 'to']),\n",
              " WordList(['road', 'to', 'learning'])]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3izBACRDyuN"
      },
      "source": [
        "### PoS(Parts of Speech)태깅\n",
        "* PoS는 품사를 의미하며, PoS 태깅은 문장 내에서 단어에 해당하는 각 품사를 태깅한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9EmhDihDxmk",
        "outputId": "d6054f0c-1c55-4a83-d1c9-01e1425c9308"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMKyeSqnD_Ro",
        "outputId": "88da9289-35df-4359-f8a2-a192b04d9d5b"
      },
      "source": [
        "words = word_tokenize(\"Think like man of action and act like man of thought.\")\n",
        "words"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Think',\n",
              " 'like',\n",
              " 'man',\n",
              " 'of',\n",
              " 'action',\n",
              " 'and',\n",
              " 'act',\n",
              " 'like',\n",
              " 'man',\n",
              " 'of',\n",
              " 'thought',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMo07ekTEG-H",
        "outputId": "bdbb9471-a6c5-414e-d35f-9465f628146d"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.pos_tag(words)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Think', 'VBP'),\n",
              " ('like', 'IN'),\n",
              " ('man', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('action', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('act', 'NN'),\n",
              " ('like', 'IN'),\n",
              " ('man', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('thought', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BflfdRIhEO32",
        "outputId": "b093fe97-b6d5-4354-bfa2-984449fa99e0"
      },
      "source": [
        "nltk.pos_tag(word_tokenize(\"A rolling stone gathers no moss\"))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('A', 'DT'),\n",
              " ('rolling', 'VBG'),\n",
              " ('stone', 'NN'),\n",
              " ('gathers', 'NNS'),\n",
              " ('no', 'DT'),\n",
              " ('moss', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaaw7lQEEfVV"
      },
      "source": [
        "* PoS 태그 리스트\n",
        "\n",
        "| Number | Tag | Description | 설명 |\n",
        "| -- | -- | -- | -- |\n",
        "| 1 | `CC` | Coordinating conjunction |\n",
        "| 2 | `CD` | Cardinal number |\n",
        "| 3 | `DT` | Determiner | 한정사\n",
        "| 4 | `EX` | Existential there |\n",
        "| 5 | `FW` | Foreign word | 외래어 |\n",
        "| 6 | `IN` | Preposition or subordinating conjunction | 전치사 또는 종속 접속사 |\n",
        "| 7 | `JJ` | Adjective | 형용사 |\n",
        "| 8 | `JJR` | Adjective, comparative | 헝용사, 비교급 |\n",
        "| 9 | `JJS` | Adjective, superlative | 형용사, 최상급 |\n",
        "| 10 | `LS` | List item marker |\n",
        "| 11 | `MD` | Modal |\n",
        "| 12 | `NN` | Noun, singular or mass | 명사, 단수형 |\n",
        "| 13 | `NNS` | Noun, plural | 명사, 복수형 |\n",
        "| 14 | `NNP` | Proper noun, singular | 고유명사, 단수형 |\n",
        "| 15 | `NNPS` | Proper noun, plural | 고유명사, 복수형 |\n",
        "| 16 | `PDT` | Predeterminer | 전치한정사 |\n",
        "| 17 | `POS` | Possessive ending | 소유형용사 |\n",
        "| 18 | `PRP` | Personal pronoun | 인칭 대명사 |\n",
        "| 19 | `PRP$` | Possessive pronoun | 소유 대명사 |\n",
        "| 20 | `RB` | Adverb | 부사 |\n",
        "| 21 | `RBR` | Adverb, comparative | 부사, 비교급 |\n",
        "| 22 | `RBS` | Adverb, superlative | 부사, 최상급 |\n",
        "| 23 | `RP` | Particle |\n",
        "| 24 | `SYM` | Symbol | 기호\n",
        "| 25 | `TO` | to |\n",
        "| 26 | `UH` | Interjection | 감탄사 |\n",
        "| 27 | `VB` | Verb, base form | 동사, 원형 |\n",
        "| 28 | `VBD` | Verb, past tense | 동사, 과거형 |\n",
        "| 29 | `VBG` | Verb, gerund or present participle | 동사, 현재분사 |\n",
        "| 30 | `VBN` | Verb, past participle | 동사, 과거분사 |\n",
        "| 31 | `VBP` | Verb, non-3rd person singular present | 동사, 비3인칭 단수 |\n",
        "| 32 | `VBZ` | Verb, 3rd person singular present | 동사, 3인칭 단수 |\n",
        "| 33 | `WDT` | Wh-determiner |\n",
        "| 34 | `WP` | Wh-pronoun |\n",
        "| 35 | `WP$` | Possessive wh-pronoun |\n",
        "| 36 | `WRB` | Wh-adverb |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x55hrxSFsX4"
      },
      "source": [
        "### 불용어 제거(stop words)\n",
        "* 영어의 전치사, 한국어의 조사 등은 분석에 필요하지 않은 경우가 많음.\n",
        "* 길이가 짧은 단어, 등장 빈도 수가 적은 단어들도 분석에 큰 영향을 주지 않음\n",
        "* 일반적으로 사용되는 도구들은 해당 단어들을 제거해주지만 완벽하게 제거되지는 않음.\n",
        "* 사용자가 불용어 사전을 만들어 해당 단어들을 제거하는 것이 좋음\n",
        "* 도구들이 걸러주지 않는 전치사, 조사 등을 불용어 사전을 만들어 불필요한 단어들을 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Rh0AOuTEb3c",
        "outputId": "31d0f29f-dfb5-4bf5-e7ae-f498504bf897"
      },
      "source": [
        "stop_words = \"on in the\" #불용어 사전 지정\n",
        "stop_words = stop_words.split(' ')\n",
        "stop_words # 각 단어가 리스트의 원소로 저장됨"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['on', 'in', 'the']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBOPnYeBGN-S",
        "outputId": "4a17e4b6-5b47-4453-aee1-85f0b5f8f52e"
      },
      "source": [
        "sentence ='singer on the stage'\n",
        "sentence = sentence.split(' ')\n",
        "nouns = []\n",
        "for noun in sentence:\n",
        "  if noun not in stop_words:\n",
        "    nouns.append(noun)\n",
        "nouns"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['singer', 'stage']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me1eSDHJGpPt"
      },
      "source": [
        "* nltk 패키지에 불용어 리스트 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEUfGxzFGbaf",
        "outputId": "aa9cc1d8-f2ee-4eee-90c6-270a60032b73"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZxrISx3G0G-",
        "outputId": "8aa64ad2-f772-4178-d801-9e1a4f5bfc36"
      },
      "source": [
        "stop_words = stopwords.words('english') #불용어를 영어로 가져옴\n",
        "print(stop_words)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5WMmy6DG7qp",
        "outputId": "0527cdc0-6aee-4ac6-ad7f-9528720bfdf1"
      },
      "source": [
        "s =\"if you do not walk today, you will have to run tomorrow.\"\n",
        "words = word_tokenize(s)\n",
        "print(words)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['if', 'you', 'do', 'not', 'walk', 'today', ',', 'you', 'will', 'have', 'to', 'run', 'tomorrow', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob6W1I4hG_2f",
        "outputId": "3c677d7f-d711-4243-eb25-638e329a3940"
      },
      "source": [
        "no_stopwords = []\n",
        "for w in words :\n",
        "  if w not in stop_words:\n",
        "    no_stopwords.append(w)\n",
        "print(no_stopwords)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['walk', 'today', ',', 'run', 'tomorrow', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuYQUGnCHcrr"
      },
      "source": [
        "### 철자 교정\n",
        "* 텍스트에 오탈자가 존재하는 경우가 있음. 예를 들어 apple을 aplpe과 같이 철자 순서가 바뀌거나 spple같이 철자가 틀릴 수 있음.\n",
        "* 사람이 적절한 추정을 통해 이해하는데는 문제가 없지만, 컴퓨터는 이러한 단어를 그대로 받아들여 처리가 필요\n",
        "* 철자 교정 알고리즘은 이미 개발되어 워드 프로세서나 다양한 서비스에서 많이 적용됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fF-i3ooiHWHV",
        "outputId": "b22aef84-9bfb-4614-cbbe-81a138514dfa"
      },
      "source": [
        "!pip install autocorrect\n",
        "from autocorrect import Speller"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.5.0.tar.gz (622 kB)\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 19.8 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 71 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 92 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 317 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 327 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 337 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 348 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 358 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 368 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 378 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 389 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 399 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 409 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 419 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 430 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 440 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 450 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 460 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 471 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 481 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 491 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 501 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 512 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 522 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 532 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 542 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 552 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 563 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 573 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 583 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 593 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 604 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 614 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 622 kB 5.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.5.0-py3-none-any.whl size=621851 sha256=a1aaa22c935c22e565ea81cfdaf9b810ab755232f7d77e80f36a73a19f997b18\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/8e/bd/f6fd900a056a031bf710a00bca338d86f43b83f0c25ab5242f\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQU9aCtYHxcn",
        "outputId": "2b1dc306-c432-41e1-d10b-ab699875b0ec"
      },
      "source": [
        "spell = Speller('en')\n",
        "print(spell('people'))\n",
        "print(spell('peope'))\n",
        "print(spell('peopae'))\n",
        "# 철자가 한번 틀렸을 때 유용하게 사용 가능"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "people\n",
            "people\n",
            "people\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvPK3X2EH9Km",
        "outputId": "37433943-9e49-4bd1-aae2-a00f340261dd"
      },
      "source": [
        "s = word_tokenize(\"Earlly biird catchess the womm.\")\n",
        "print(s)\n",
        "ss = ' '.join([spell(s) for s in s]) #spell에 통과시켜보기\n",
        "print(ss)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Earlly', 'biird', 'catchess', 'the', 'womm', '.']\n",
            "Early bird catches the worm .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSZ2sEE8IYcD"
      },
      "source": [
        "### 언어의 단수화와 **복수화**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W3NrYCRIVgd",
        "outputId": "9a5b0e19-2093-41dd-c6cb-c2609ba2f9cb"
      },
      "source": [
        "from textblob import TextBlob\n",
        "words = 'apples bannas oranges'\n",
        "tb = TextBlob(words)\n",
        "print(tb.words)\n",
        "print(tb.words.singularize()) #복수 명사들을 단수화 취급해 짐"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['apples', 'bannas', 'oranges']\n",
            "['apple', 'banna', 'orange']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63KH0Ug8Iyp5",
        "outputId": "234c763d-c313-49a1-d2b1-6cb582bf4093"
      },
      "source": [
        "words = 'car train airplane'\n",
        "tb = TextBlob(words)\n",
        "print(tb.words)\n",
        "print(tb.words.pluralize()) #복수화"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['car', 'train', 'airplane']\n",
            "['cars', 'trains', 'airplanes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMk4mVVdIrkJ"
      },
      "source": [
        "### 어간(Stemming) 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTg8yP3jIqm3"
      },
      "source": [
        "import nltk\n",
        "\n",
        "stemmer = nltk.stem.PorterStemmer()"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "I4BplDlcJFEh",
        "outputId": "2ca035de-ac35-4c1d-f1a1-7849477fd612"
      },
      "source": [
        "stemmer.stem('application') # 어간만 출력 됨."
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'applic'"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4L58ShRsJINc",
        "outputId": "9c611922-c3bd-4e7f-ff88-e507c4bd17df"
      },
      "source": [
        "stemmer.stem('beginning')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'begin'"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SRthdqGXJOgI",
        "outputId": "ac85ebe9-7229-4199-ebd5-a4438f29262c"
      },
      "source": [
        "stemmer.stem('catches')"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'catch'"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2IzJrSmjJUYX",
        "outputId": "66511b9b-e3c9-43ab-ce80-3d02ebc41034"
      },
      "source": [
        "stemmer.stem('education')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'educ'"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu31eRp3JY1T"
      },
      "source": [
        "### 표제어(Lemmatization) 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WVHD-tpJX6r",
        "outputId": "05a48685-2a4f-49ad-b3b7-bbf581951cce"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "emYK2xSMJkBC",
        "outputId": "bcbc0599-f83d-4448-dd1e-d9b6f1a04d7f"
      },
      "source": [
        "lemmatizer.lemmatize('applications') #사전에 등록된 표제어 추출"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'application'"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xKifVncxJvkc",
        "outputId": "02692a7b-1b47-4eae-adee-ceaf00f179b0"
      },
      "source": [
        "lemmatizer.lemmatize('catches')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'catch'"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8wLgyM8KXi0"
      },
      "source": [
        "### 개체명 인식(Named Entity Recognition\n",
        "* 예를들어 서울같은 고유명사들을 개체라하며 이를 추출해 보겠다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heRQr57kJ8ou",
        "outputId": "33bae8ac-8e09-4516-9424-4c08cf13806a"
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UboZwIAqKpcR",
        "outputId": "1070e515-e82f-4591-b49d-5221bb5594e5"
      },
      "source": [
        "s = \"Rome was not built in a day\"\n",
        "print(s)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rome was not built in a day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8_ymMnxKtxv",
        "outputId": "cc7a62b0-1c06-4d9a-84c5-abd3c4c5279b"
      },
      "source": [
        "tages = nltk.pos_tag(word_tokenize(s)) # 품사 뽑아서 tages에 저장\n",
        "print(tages)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Rome', 'NNP'), ('was', 'VBD'), ('not', 'RB'), ('built', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('day', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCkkQ0r7Kzni",
        "outputId": "535e9de8-bbe7-423a-9488-b40df0bf51e5"
      },
      "source": [
        "entities = nltk.ne_chunk(tages, binary = True)\n",
        "print(entities) # Rome에만 NE가 붙음, 즉 개체명사를 인식한다."
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S (NE Rome/NNP) was/VBD not/RB built/VBN in/IN a/DT day/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktcy8RmsLfBB"
      },
      "source": [
        "### 단어 중의성(Lexical Ambiguity)\n",
        "* 애매모호한 단어들"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK7LfX5-K-dl",
        "outputId": "22a97a70-2d70-4ee9-bb18-3f247f4c9e5a"
      },
      "source": [
        "import nltk\n",
        "from nltk.wsd import lesk\n",
        "s='I saw bats.' #나는 박쥐를 보았다 또는 나는 야구방망이를 보았다 \n",
        "# 문장의 중의성을 이 문장만 가지고 판단하기 힘듦.\n",
        "# 어떤의미에 가까운지 찾아내는 방법이 있다.\n",
        "\n",
        "print(word_tokenize(s))\n",
        "# lesk : 단어가 가지는 의미를 알려달라\n",
        "print(lesk(word_tokenize(s), 'saw')) \n",
        "# -> 결과 : saw를 동사로 봐라.\n",
        "print(lesk(word_tokenize(s), 'bats'))\n",
        "# -> 결과 : 명사형태로 야구방망이의 형태로 봐라.\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'saw', 'bats', '.']\n",
            "Synset('saw.v.01')\n",
            "Synset('squash_racket.n.01')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlFXn1-8MaVm"
      },
      "source": [
        "## 한국어 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQqKnnE4Mca-"
      },
      "source": [
        "### 정규표현식\n",
        "* 한국어 정규 표현식도 대부분의 문법은 영어 정규 표현식과 같음\n",
        "* 한국어는 자음과 모음이 분리되어 있기 때문에, 문법을 지정할 때는 자음과 모음을 동시에 고려해야 함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aH3f5veMozf"
      },
      "source": [
        "### match\n",
        "* 컴파일한 정규 표현식을 이용해 문자열이 정규 표현식과 맞는지 검사"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyvlmJOHMEpM",
        "outputId": "f2474658-890e-4819-973d-51511d33330f"
      },
      "source": [
        "import re\n",
        "check = '[ㄱ-ㅎ]+'\n",
        "\n",
        "print(re.match(check, 'ㅎ 안녕하세요.')) #처음부터 ㅎ이 있어서 리턴됨.\n",
        "print(re.match(check, '안녕하세요. ㅎ')) #처음에 안녕하세요 부터 나와서 리턴 안됨."
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 1), match='ㅎ'>\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIoao4C8NcNX"
      },
      "source": [
        "### search\n",
        "* match와 다르게 search는 문자열의 전체를 검사"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFw9RRPTNQgO",
        "outputId": "75b4793f-2df8-42ee-ca1d-980072f54a7a"
      },
      "source": [
        "check = '[ㄱ-ㅎ:ㅏ-ㅣ]+'\n",
        "\n",
        "print(re.search(check, 'ㄱㅏ 안녕하세요'))\n",
        "print(re.match(check, '안 ㄱㅏ')) #match는 맨 처음 단어가 조건에 맞지 않으면 none출력\n",
        "print(re.search(check, '안 ㄱㅏ')) #search는 전체를 검사하기 떄문에 ㄱㅏ를 출력함."
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 2), match='ㄱㅏ'>\n",
            "None\n",
            "<re.Match object; span=(2, 4), match='ㄱㅏ'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fP-YrZuOHDB"
      },
      "source": [
        "### sub\n",
        "* 정규 표현식과 일치하는 부분을 다른 문자열로 교체"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpvn4xAAN2N0",
        "outputId": "3cf4d6ef-f1bb-4d0b-e759-200f5e99bc88"
      },
      "source": [
        "print(re.sub('[가-힣]','가나다라마바사','1')) # [가-힣]는 한글의 모든 문자열 조합을 뜻함\n",
        "print(re.sub('[^가-힣]', '가나다라마바사','1'))\n",
        "# 조건에 맞으면 가나다라마바사를 1로 바꾼다. 두번째는 ^로 제외했으므로 그대로 출력"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "가나다라마바사\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwHGLcCZOtgR"
      },
      "source": [
        "### 토큰화(Tokenization)\n",
        "* 한국어를 학습 데이터를 사용할 떄는 언어의 특성으로 인해 추가로 고려해야 할 사항이 존재\n",
        "* 한국어는 띄어쓰기를 준수하지 않아도 의미가 전닳되는 경우가 많아 띄어쓰기가 지켜지지 않을 가능성이 존재\n",
        "* 띄어쓰기가 지켜지지 않으면 정상적인 토큰 분리가 이루어지지 않음\n",
        "* 한국어는 형태로라는 개념이 존재해 추가로 고려해주어야만 함\n",
        "* '그는', '그가' 등의 단어들은 같은 의미를 가리키지만 텍스트 처리에서는 다르게 받아들일 수 있어 처리를 해줘야 함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxLaKWqvPFjF"
      },
      "source": [
        "### 한국어 자연어 처리 konlpy와 형태소 분석기 MeCab 설치\n",
        "* https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKPRtnoYOf-L",
        "outputId": "23f8d4c4-50cf-4ae1-bfd6-b0dda38ce88d"
      },
      "source": [
        "!set -x \\\n",
        "&& pip install konlpy \\\n",
        "&& curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh | bash -x\n",
        "# 쉘을 설치"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+ pip install konlpy\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 44.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n",
            "+ curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh\n",
            "+ bash -x\n",
            "+ mecab_dicdir=/usr/local/lib/mecab/dic/mecab-ko-dic\n",
            "+ set -e\n",
            "++ uname\n",
            "+ os=Linux\n",
            "+ [[ ! Linux == \\L\\i\\n\\u\\x ]]\n",
            "+ hash sudo\n",
            "+ sudo=sudo\n",
            "+ python=python3\n",
            "+ hash pyenv\n",
            "+ at_user_site=\n",
            "++ check_python_site_location_is_writable\n",
            "++ python3 -\n",
            "+ [[ 1 == \\0 ]]\n",
            "+ hash automake\n",
            "+ echo 'Installing automake (A dependency for mecab-ko)'\n",
            "Installing automake (A dependency for mecab-ko)\n",
            "+ install_automake\n",
            "+ '[' Linux == Linux ']'\n",
            "++ grep -Ei 'debian|buntu|mint' /etc/lsb-release /etc/os-release\n",
            "+ '[' '/etc/lsb-release:DISTRIB_ID=Ubuntu\n",
            "/etc/lsb-release:DISTRIB_DESCRIPTION=\"Ubuntu 18.04.5 LTS\"\n",
            "/etc/os-release:NAME=\"Ubuntu\"\n",
            "/etc/os-release:ID=ubuntu\n",
            "/etc/os-release:ID_LIKE=debian\n",
            "/etc/os-release:PRETTY_NAME=\"Ubuntu 18.04.5 LTS\"\n",
            "/etc/os-release:HOME_URL=\"https://www.ubuntu.com/\"\n",
            "/etc/os-release:SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "/etc/os-release:BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "/etc/os-release:PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "/etc/os-release:UBUNTU_CODENAME=bionic' ']'\n",
            "+ sudo apt-get update\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [67.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,428 kB]\n",
            "Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [594 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,351 kB]\n",
            "Hit:17 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:19 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,802 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [627 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,202 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,787 kB]\n",
            "Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [922 kB]\n",
            "Get:25 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [42.0 kB]\n",
            "Fetched 13.1 MB in 3s (4,251 kB/s)\n",
            "Reading package lists... Done\n",
            "+ sudo apt-get install -y automake\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  autoconf autotools-dev libsigsegv2 m4\n",
            "Suggested packages:\n",
            "  autoconf-archive gnu-standards autoconf-doc libtool gettext m4-doc\n",
            "The following NEW packages will be installed:\n",
            "  autoconf automake autotools-dev libsigsegv2 m4\n",
            "0 upgraded, 5 newly installed, 0 to remove and 79 not upgraded.\n",
            "Need to get 1,082 kB of archives.\n",
            "After this operation, 3,994 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigsegv2 amd64 2.12-1 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 m4 amd64 1.4.18-1 [197 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 autoconf all 2.69-11 [322 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 autotools-dev all 20180224.1 [39.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 automake all 1:1.15.1-3ubuntu2 [509 kB]\n",
            "Fetched 1,082 kB in 1s (1,010 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libsigsegv2:amd64.\n",
            "(Reading database ... 155013 files and directories currently installed.)\n",
            "Preparing to unpack .../libsigsegv2_2.12-1_amd64.deb ...\n",
            "Unpacking libsigsegv2:amd64 (2.12-1) ...\n",
            "Selecting previously unselected package m4.\n",
            "Preparing to unpack .../archives/m4_1.4.18-1_amd64.deb ...\n",
            "Unpacking m4 (1.4.18-1) ...\n",
            "Selecting previously unselected package autoconf.\n",
            "Preparing to unpack .../autoconf_2.69-11_all.deb ...\n",
            "Unpacking autoconf (2.69-11) ...\n",
            "Selecting previously unselected package autotools-dev.\n",
            "Preparing to unpack .../autotools-dev_20180224.1_all.deb ...\n",
            "Unpacking autotools-dev (20180224.1) ...\n",
            "Selecting previously unselected package automake.\n",
            "Preparing to unpack .../automake_1%3a1.15.1-3ubuntu2_all.deb ...\n",
            "Unpacking automake (1:1.15.1-3ubuntu2) ...\n",
            "Setting up libsigsegv2:amd64 (2.12-1) ...\n",
            "Setting up m4 (1.4.18-1) ...\n",
            "Setting up autotools-dev (20180224.1) ...\n",
            "Setting up autoconf (2.69-11) ...\n",
            "Setting up automake (1:1.15.1-3ubuntu2) ...\n",
            "update-alternatives: using /usr/bin/automake-1.15 to provide /usr/bin/automake (automake) in auto mode\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "+ hash mecab\n",
            "+ echo 'Install mecab-ko-dic'\n",
            "Install mecab-ko-dic\n",
            "+ install_mecab_ko\n",
            "+ cd /tmp\n",
            "+ curl -LO https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1381k  100 1381k    0     0  2122k      0 --:--:-- --:--:-- --:--:-- 2122k\n",
            "+ tar zxfv mecab-0.996-ko-0.9.2.tar.gz\n",
            "mecab-0.996-ko-0.9.2/\n",
            "mecab-0.996-ko-0.9.2/example/\n",
            "mecab-0.996-ko-0.9.2/example/example.cpp\n",
            "mecab-0.996-ko-0.9.2/example/example_lattice.cpp\n",
            "mecab-0.996-ko-0.9.2/example/example_lattice.c\n",
            "mecab-0.996-ko-0.9.2/example/example.c\n",
            "mecab-0.996-ko-0.9.2/example/thread_test.cpp\n",
            "mecab-0.996-ko-0.9.2/mecab-config.in\n",
            "mecab-0.996-ko-0.9.2/man/\n",
            "mecab-0.996-ko-0.9.2/man/Makefile.am\n",
            "mecab-0.996-ko-0.9.2/man/mecab.1\n",
            "mecab-0.996-ko-0.9.2/man/Makefile.in\n",
            "mecab-0.996-ko-0.9.2/mecab.iss.in\n",
            "mecab-0.996-ko-0.9.2/config.guess\n",
            "mecab-0.996-ko-0.9.2/README\n",
            "mecab-0.996-ko-0.9.2/COPYING\n",
            "mecab-0.996-ko-0.9.2/CHANGES.md\n",
            "mecab-0.996-ko-0.9.2/README.md\n",
            "mecab-0.996-ko-0.9.2/INSTALL\n",
            "mecab-0.996-ko-0.9.2/config.sub\n",
            "mecab-0.996-ko-0.9.2/configure.in\n",
            "mecab-0.996-ko-0.9.2/swig/\n",
            "mecab-0.996-ko-0.9.2/swig/Makefile\n",
            "mecab-0.996-ko-0.9.2/swig/version.h.in\n",
            "mecab-0.996-ko-0.9.2/swig/version.h\n",
            "mecab-0.996-ko-0.9.2/swig/MeCab.i\n",
            "mecab-0.996-ko-0.9.2/aclocal.m4\n",
            "mecab-0.996-ko-0.9.2/LGPL\n",
            "mecab-0.996-ko-0.9.2/Makefile.am\n",
            "mecab-0.996-ko-0.9.2/configure\n",
            "mecab-0.996-ko-0.9.2/tests/\n",
            "mecab-0.996-ko-0.9.2/tests/autolink/\n",
            "mecab-0.996-ko-0.9.2/tests/autolink/unk.def\n",
            "mecab-0.996-ko-0.9.2/tests/autolink/dicrc\n",
            "mecab-0.996-ko-0.9.2/tests/autolink/dic.csv\n",
            "mecab-0.996-ko-0.9.2/tests/autolink/test\n",
            "mecab-0.996-ko-0.9.2/tests/autolink/char.def\n",
            "mecab-0.996-ko-0.9.2/tests/autolink/matrix.def\n",
            "mecab-0.996-ko-0.9.2/tests/autolink/test.gld\n",
            "mecab-0.996-ko-0.9.2/tests/t9/\n",
            "mecab-0.996-ko-0.9.2/tests/t9/unk.def\n",
            "mecab-0.996-ko-0.9.2/tests/t9/ipadic.pl\n",
            "mecab-0.996-ko-0.9.2/tests/t9/dicrc\n",
            "mecab-0.996-ko-0.9.2/tests/t9/dic.csv\n",
            "mecab-0.996-ko-0.9.2/tests/t9/test\n",
            "mecab-0.996-ko-0.9.2/tests/t9/char.def\n",
            "mecab-0.996-ko-0.9.2/tests/t9/matrix.def\n",
            "mecab-0.996-ko-0.9.2/tests/t9/mkdic.pl\n",
            "mecab-0.996-ko-0.9.2/tests/t9/test.gld\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/ipa.train\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/ipa.test\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/seed/\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/seed/rewrite.def\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/seed/feature.def\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/seed/unk.def\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/seed/dicrc\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/seed/dic.csv\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/seed/char.def\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/seed/matrix.def\n",
            "mecab-0.996-ko-0.9.2/tests/run-eval.sh\n",
            "mecab-0.996-ko-0.9.2/tests/run-cost-train.sh\n",
            "mecab-0.996-ko-0.9.2/tests/Makefile.am\n",
            "mecab-0.996-ko-0.9.2/tests/katakana/\n",
            "mecab-0.996-ko-0.9.2/tests/katakana/unk.def\n",
            "mecab-0.996-ko-0.9.2/tests/katakana/dicrc\n",
            "mecab-0.996-ko-0.9.2/tests/katakana/dic.csv\n",
            "mecab-0.996-ko-0.9.2/tests/katakana/test\n",
            "mecab-0.996-ko-0.9.2/tests/katakana/char.def\n",
            "mecab-0.996-ko-0.9.2/tests/katakana/matrix.def\n",
            "mecab-0.996-ko-0.9.2/tests/katakana/test.gld\n",
            "mecab-0.996-ko-0.9.2/tests/eval/\n",
            "mecab-0.996-ko-0.9.2/tests/eval/answer\n",
            "mecab-0.996-ko-0.9.2/tests/eval/system\n",
            "mecab-0.996-ko-0.9.2/tests/eval/test.gld\n",
            "mecab-0.996-ko-0.9.2/tests/shiin/\n",
            "mecab-0.996-ko-0.9.2/tests/shiin/unk.def\n",
            "mecab-0.996-ko-0.9.2/tests/shiin/dicrc\n",
            "mecab-0.996-ko-0.9.2/tests/shiin/dic.csv\n",
            "mecab-0.996-ko-0.9.2/tests/shiin/test\n",
            "mecab-0.996-ko-0.9.2/tests/shiin/char.def\n",
            "mecab-0.996-ko-0.9.2/tests/shiin/matrix.def\n",
            "mecab-0.996-ko-0.9.2/tests/shiin/mkdic.pl\n",
            "mecab-0.996-ko-0.9.2/tests/shiin/test.gld\n",
            "mecab-0.996-ko-0.9.2/tests/latin/\n",
            "mecab-0.996-ko-0.9.2/tests/latin/unk.def\n",
            "mecab-0.996-ko-0.9.2/tests/latin/dicrc\n",
            "mecab-0.996-ko-0.9.2/tests/latin/dic.csv\n",
            "mecab-0.996-ko-0.9.2/tests/latin/test\n",
            "mecab-0.996-ko-0.9.2/tests/latin/char.def\n",
            "mecab-0.996-ko-0.9.2/tests/latin/matrix.def\n",
            "mecab-0.996-ko-0.9.2/tests/latin/test.gld\n",
            "mecab-0.996-ko-0.9.2/tests/chartype/\n",
            "mecab-0.996-ko-0.9.2/tests/chartype/unk.def\n",
            "mecab-0.996-ko-0.9.2/tests/chartype/dicrc\n",
            "mecab-0.996-ko-0.9.2/tests/chartype/dic.csv\n",
            "mecab-0.996-ko-0.9.2/tests/chartype/test\n",
            "mecab-0.996-ko-0.9.2/tests/chartype/char.def\n",
            "mecab-0.996-ko-0.9.2/tests/chartype/matrix.def\n",
            "mecab-0.996-ko-0.9.2/tests/chartype/test.gld\n",
            "mecab-0.996-ko-0.9.2/tests/run-dics.sh\n",
            "mecab-0.996-ko-0.9.2/tests/ngram/\n",
            "mecab-0.996-ko-0.9.2/tests/ngram/unk.def\n",
            "mecab-0.996-ko-0.9.2/tests/ngram/dicrc\n",
            "mecab-0.996-ko-0.9.2/tests/ngram/dic.csv\n",
            "mecab-0.996-ko-0.9.2/tests/ngram/test\n",
            "mecab-0.996-ko-0.9.2/tests/ngram/char.def\n",
            "mecab-0.996-ko-0.9.2/tests/ngram/matrix.def\n",
            "mecab-0.996-ko-0.9.2/tests/ngram/test.gld\n",
            "mecab-0.996-ko-0.9.2/tests/Makefile.in\n",
            "mecab-0.996-ko-0.9.2/ltmain.sh\n",
            "mecab-0.996-ko-0.9.2/config.rpath\n",
            "mecab-0.996-ko-0.9.2/config.h.in\n",
            "mecab-0.996-ko-0.9.2/mecabrc.in\n",
            "mecab-0.996-ko-0.9.2/GPL\n",
            "mecab-0.996-ko-0.9.2/Makefile.train\n",
            "mecab-0.996-ko-0.9.2/ChangeLog\n",
            "mecab-0.996-ko-0.9.2/install-sh\n",
            "mecab-0.996-ko-0.9.2/AUTHORS\n",
            "mecab-0.996-ko-0.9.2/doc/\n",
            "mecab-0.996-ko-0.9.2/doc/bindings.html\n",
            "mecab-0.996-ko-0.9.2/doc/posid.html\n",
            "mecab-0.996-ko-0.9.2/doc/unk.html\n",
            "mecab-0.996-ko-0.9.2/doc/learn.html\n",
            "mecab-0.996-ko-0.9.2/doc/format.html\n",
            "mecab-0.996-ko-0.9.2/doc/libmecab.html\n",
            "mecab-0.996-ko-0.9.2/doc/mecab.css\n",
            "mecab-0.996-ko-0.9.2/doc/feature.html\n",
            "mecab-0.996-ko-0.9.2/doc/Makefile.am\n",
            "mecab-0.996-ko-0.9.2/doc/soft.html\n",
            "mecab-0.996-ko-0.9.2/doc/en/\n",
            "mecab-0.996-ko-0.9.2/doc/en/bindings.html\n",
            "mecab-0.996-ko-0.9.2/doc/dic-detail.html\n",
            "mecab-0.996-ko-0.9.2/doc/flow.png\n",
            "mecab-0.996-ko-0.9.2/doc/mecab.html\n",
            "mecab-0.996-ko-0.9.2/doc/index.html\n",
            "mecab-0.996-ko-0.9.2/doc/result.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/tab_a.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/globals_eval.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Tagger-members.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/functions_vars.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/doxygen.css\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/tab_r.gif\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Lattice.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/functions.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Tagger.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/mecab_8h_source.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/tabs.css\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/nav_f.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/tab_b.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/globals.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/nav_h.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/tab_h.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Model.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/globals_func.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/closed.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/tab_l.gif\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/structmecab__path__t-members.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/functions_func.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/globals_type.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Lattice-members.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/structmecab__node__t.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/namespacemembers_func.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/tab_s.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/structmecab__dictionary__info__t-members.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/namespacemembers_type.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Model-members.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/structmecab__dictionary__info__t.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/namespaces.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/namespacemembers.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/namespaceMeCab.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/structmecab__path__t.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/files.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/structmecab__node__t-members.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/index.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/annotated.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/globals_defs.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/classes.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/mecab_8h-source.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/doxygen.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/tab_b.gif\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/bc_s.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/open.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/mecab_8h.html\n",
            "mecab-0.996-ko-0.9.2/doc/dic.html\n",
            "mecab-0.996-ko-0.9.2/doc/partial.html\n",
            "mecab-0.996-ko-0.9.2/doc/feature.png\n",
            "mecab-0.996-ko-0.9.2/doc/Makefile.in\n",
            "mecab-0.996-ko-0.9.2/missing\n",
            "mecab-0.996-ko-0.9.2/BSD\n",
            "mecab-0.996-ko-0.9.2/NEWS\n",
            "mecab-0.996-ko-0.9.2/mkinstalldirs\n",
            "mecab-0.996-ko-0.9.2/src/\n",
            "mecab-0.996-ko-0.9.2/src/dictionary.h\n",
            "mecab-0.996-ko-0.9.2/src/writer.h\n",
            "mecab-0.996-ko-0.9.2/src/utils.h\n",
            "mecab-0.996-ko-0.9.2/src/string_buffer.cpp\n",
            "mecab-0.996-ko-0.9.2/src/tokenizer.cpp\n",
            "mecab-0.996-ko-0.9.2/src/make.bat\n",
            "mecab-0.996-ko-0.9.2/src/mecab.h\n",
            "mecab-0.996-ko-0.9.2/src/freelist.h\n",
            "mecab-0.996-ko-0.9.2/src/string_buffer.h\n",
            "mecab-0.996-ko-0.9.2/src/learner_tagger.h\n",
            "mecab-0.996-ko-0.9.2/src/dictionary_compiler.cpp\n",
            "mecab-0.996-ko-0.9.2/src/eval.cpp\n",
            "mecab-0.996-ko-0.9.2/src/mecab-system-eval.cpp\n",
            "mecab-0.996-ko-0.9.2/src/darts.h\n",
            "mecab-0.996-ko-0.9.2/src/param.h\n",
            "mecab-0.996-ko-0.9.2/src/char_property.h\n",
            "mecab-0.996-ko-0.9.2/src/learner_node.h\n",
            "mecab-0.996-ko-0.9.2/src/mecab-dict-gen.cpp\n",
            "mecab-0.996-ko-0.9.2/src/mecab-dict-index.cpp\n",
            "mecab-0.996-ko-0.9.2/src/winmain.h\n",
            "mecab-0.996-ko-0.9.2/src/thread.h\n",
            "mecab-0.996-ko-0.9.2/src/context_id.cpp\n",
            "mecab-0.996-ko-0.9.2/src/Makefile.am\n",
            "mecab-0.996-ko-0.9.2/src/connector.h\n",
            "mecab-0.996-ko-0.9.2/src/common.h\n",
            "mecab-0.996-ko-0.9.2/src/dictionary_rewriter.cpp\n",
            "mecab-0.996-ko-0.9.2/src/Makefile.msvc.in\n",
            "mecab-0.996-ko-0.9.2/src/dictionary_rewriter.h\n",
            "mecab-0.996-ko-0.9.2/src/feature_index.h\n",
            "mecab-0.996-ko-0.9.2/src/iconv_utils.cpp\n",
            "mecab-0.996-ko-0.9.2/src/char_property.cpp\n",
            "mecab-0.996-ko-0.9.2/src/mecab-test-gen.cpp\n",
            "mecab-0.996-ko-0.9.2/src/tagger.cpp\n",
            "mecab-0.996-ko-0.9.2/src/mecab-cost-train.cpp\n",
            "mecab-0.996-ko-0.9.2/src/learner.cpp\n",
            "mecab-0.996-ko-0.9.2/src/dictionary.cpp\n",
            "mecab-0.996-ko-0.9.2/src/lbfgs.cpp\n",
            "mecab-0.996-ko-0.9.2/src/ucs.h\n",
            "mecab-0.996-ko-0.9.2/src/writer.cpp\n",
            "mecab-0.996-ko-0.9.2/src/learner_tagger.cpp\n",
            "mecab-0.996-ko-0.9.2/src/lbfgs.h\n",
            "mecab-0.996-ko-0.9.2/src/libmecab.cpp\n",
            "mecab-0.996-ko-0.9.2/src/tokenizer.h\n",
            "mecab-0.996-ko-0.9.2/src/mecab.cpp\n",
            "mecab-0.996-ko-0.9.2/src/utils.cpp\n",
            "mecab-0.996-ko-0.9.2/src/dictionary_generator.cpp\n",
            "mecab-0.996-ko-0.9.2/src/param.cpp\n",
            "mecab-0.996-ko-0.9.2/src/context_id.h\n",
            "mecab-0.996-ko-0.9.2/src/mmap.h\n",
            "mecab-0.996-ko-0.9.2/src/viterbi.h\n",
            "mecab-0.996-ko-0.9.2/src/viterbi.cpp\n",
            "mecab-0.996-ko-0.9.2/src/stream_wrapper.h\n",
            "mecab-0.996-ko-0.9.2/src/feature_index.cpp\n",
            "mecab-0.996-ko-0.9.2/src/nbest_generator.h\n",
            "mecab-0.996-ko-0.9.2/src/ucstable.h\n",
            "mecab-0.996-ko-0.9.2/src/nbest_generator.cpp\n",
            "mecab-0.996-ko-0.9.2/src/iconv_utils.h\n",
            "mecab-0.996-ko-0.9.2/src/connector.cpp\n",
            "mecab-0.996-ko-0.9.2/src/Makefile.in\n",
            "mecab-0.996-ko-0.9.2/src/scoped_ptr.h\n",
            "mecab-0.996-ko-0.9.2/Makefile.in\n",
            "+ cd mecab-0.996-ko-0.9.2\n",
            "+ ./configure\n",
            "checking for a BSD-compatible install... /usr/bin/install -c\n",
            "checking whether build environment is sane... yes\n",
            "checking for a thread-safe mkdir -p... /bin/mkdir -p\n",
            "checking for gawk... no\n",
            "checking for mawk... mawk\n",
            "checking whether make sets $(MAKE)... yes\n",
            "checking for gcc... gcc\n",
            "checking whether the C compiler works... yes\n",
            "checking for C compiler default output file name... a.out\n",
            "checking for suffix of executables... \n",
            "checking whether we are cross compiling... no\n",
            "checking for suffix of object files... o\n",
            "checking whether we are using the GNU C compiler... yes\n",
            "checking whether gcc accepts -g... yes\n",
            "checking for gcc option to accept ISO C89... none needed\n",
            "checking for style of include used by make... GNU\n",
            "checking dependency style of gcc... none\n",
            "checking for g++... g++\n",
            "checking whether we are using the GNU C++ compiler... yes\n",
            "checking whether g++ accepts -g... yes\n",
            "checking dependency style of g++... none\n",
            "checking how to run the C preprocessor... gcc -E\n",
            "checking for grep that handles long lines and -e... /bin/grep\n",
            "checking for egrep... /bin/grep -E\n",
            "checking whether gcc needs -traditional... no\n",
            "checking whether make sets $(MAKE)... (cached) yes\n",
            "checking build system type... x86_64-unknown-linux-gnu\n",
            "checking host system type... x86_64-unknown-linux-gnu\n",
            "checking how to print strings... printf\n",
            "checking for a sed that does not truncate output... /bin/sed\n",
            "checking for fgrep... /bin/grep -F\n",
            "checking for ld used by gcc... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B\n",
            "checking the name lister (/usr/bin/nm -B) interface... BSD nm\n",
            "checking whether ln -s works... yes\n",
            "checking the maximum length of command line arguments... 1572864\n",
            "checking whether the shell understands some XSI constructs... yes\n",
            "checking whether the shell understands \"+=\"... yes\n",
            "checking how to convert x86_64-unknown-linux-gnu file names to x86_64-unknown-linux-gnu format... func_convert_file_noop\n",
            "checking how to convert x86_64-unknown-linux-gnu file names to toolchain format... func_convert_file_noop\n",
            "checking for /usr/bin/ld option to reload object files... -r\n",
            "checking for objdump... objdump\n",
            "checking how to recognize dependent libraries... pass_all\n",
            "checking for dlltool... dlltool\n",
            "checking how to associate runtime and link libraries... printf %s\\n\n",
            "checking for ar... ar\n",
            "checking for archiver @FILE support... @\n",
            "checking for strip... strip\n",
            "checking for ranlib... ranlib\n",
            "checking command to parse /usr/bin/nm -B output from gcc object... ok\n",
            "checking for sysroot... no\n",
            "./configure: line 7378: /usr/bin/file: No such file or directory\n",
            "checking for mt... no\n",
            "checking if : is a manifest tool... no\n",
            "checking for ANSI C header files... yes\n",
            "checking for sys/types.h... yes\n",
            "checking for sys/stat.h... yes\n",
            "checking for stdlib.h... yes\n",
            "checking for string.h... yes\n",
            "checking for memory.h... yes\n",
            "checking for strings.h... yes\n",
            "checking for inttypes.h... yes\n",
            "checking for stdint.h... yes\n",
            "checking for unistd.h... yes\n",
            "checking for dlfcn.h... yes\n",
            "checking for objdir... .libs\n",
            "checking if gcc supports -fno-rtti -fno-exceptions... no\n",
            "checking for gcc option to produce PIC... -fPIC -DPIC\n",
            "checking if gcc PIC flag -fPIC -DPIC works... yes\n",
            "checking if gcc static flag -static works... yes\n",
            "checking if gcc supports -c -o file.o... yes\n",
            "checking if gcc supports -c -o file.o... (cached) yes\n",
            "checking whether the gcc linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking whether -lc should be explicitly linked in... no\n",
            "checking dynamic linker characteristics... GNU/Linux ld.so\n",
            "checking how to hardcode library paths into programs... immediate\n",
            "checking whether stripping libraries is possible... yes\n",
            "checking if libtool supports shared libraries... yes\n",
            "checking whether to build shared libraries... yes\n",
            "checking whether to build static libraries... yes\n",
            "checking how to run the C++ preprocessor... g++ -E\n",
            "checking for ld used by g++... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking whether the g++ linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking for g++ option to produce PIC... -fPIC -DPIC\n",
            "checking if g++ PIC flag -fPIC -DPIC works... yes\n",
            "checking if g++ static flag -static works... yes\n",
            "checking if g++ supports -c -o file.o... yes\n",
            "checking if g++ supports -c -o file.o... (cached) yes\n",
            "checking whether the g++ linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking dynamic linker characteristics... (cached) GNU/Linux ld.so\n",
            "checking how to hardcode library paths into programs... immediate\n",
            "checking for library containing strerror... none required\n",
            "checking whether byte ordering is bigendian... no\n",
            "checking for ld used by GCC... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking for shared library run path origin... done\n",
            "checking for iconv... yes\n",
            "checking for working iconv... yes\n",
            "checking for iconv declaration... \n",
            "         extern size_t iconv (iconv_t cd, char * *inbuf, size_t *inbytesleft, char * *outbuf, size_t *outbytesleft);\n",
            "checking for ANSI C header files... (cached) yes\n",
            "checking for an ANSI C-conforming const... yes\n",
            "checking whether byte ordering is bigendian... (cached) no\n",
            "checking for string.h... (cached) yes\n",
            "checking for stdlib.h... (cached) yes\n",
            "checking for unistd.h... (cached) yes\n",
            "checking fcntl.h usability... yes\n",
            "checking fcntl.h presence... yes\n",
            "checking for fcntl.h... yes\n",
            "checking for stdint.h... (cached) yes\n",
            "checking for sys/stat.h... (cached) yes\n",
            "checking sys/mman.h usability... yes\n",
            "checking sys/mman.h presence... yes\n",
            "checking for sys/mman.h... yes\n",
            "checking sys/times.h usability... yes\n",
            "checking sys/times.h presence... yes\n",
            "checking for sys/times.h... yes\n",
            "checking for sys/types.h... (cached) yes\n",
            "checking dirent.h usability... yes\n",
            "checking dirent.h presence... yes\n",
            "checking for dirent.h... yes\n",
            "checking ctype.h usability... yes\n",
            "checking ctype.h presence... yes\n",
            "checking for ctype.h... yes\n",
            "checking for sys/types.h... (cached) yes\n",
            "checking io.h usability... no\n",
            "checking io.h presence... no\n",
            "checking for io.h... no\n",
            "checking windows.h usability... no\n",
            "checking windows.h presence... no\n",
            "checking for windows.h... no\n",
            "checking pthread.h usability... yes\n",
            "checking pthread.h presence... yes\n",
            "checking for pthread.h... yes\n",
            "checking for off_t... yes\n",
            "checking for size_t... yes\n",
            "checking size of char... 1\n",
            "checking size of short... 2\n",
            "checking size of int... 4\n",
            "checking size of long... 8\n",
            "checking size of long long... 8\n",
            "checking size of size_t... 8\n",
            "checking for size_t... (cached) yes\n",
            "checking for unsigned long long int... yes\n",
            "checking for stdlib.h... (cached) yes\n",
            "checking for unistd.h... (cached) yes\n",
            "checking for sys/param.h... yes\n",
            "checking for getpagesize... yes\n",
            "checking for working mmap... yes\n",
            "checking for main in -lstdc++... yes\n",
            "checking for pthread_create in -lpthread... yes\n",
            "checking for pthread_join in -lpthread... yes\n",
            "checking for getenv... yes\n",
            "checking for opendir... yes\n",
            "checking whether make is GNU Make... yes\n",
            "checking if g++ supports stl <vector> (required)... yes\n",
            "checking if g++ supports stl <list> (required)... yes\n",
            "checking if g++ supports stl <map> (required)... yes\n",
            "checking if g++ supports stl <set> (required)... yes\n",
            "checking if g++ supports stl <queue> (required)... yes\n",
            "checking if g++ supports stl <functional> (required)... yes\n",
            "checking if g++ supports stl <algorithm> (required)... yes\n",
            "checking if g++ supports stl <string> (required)... yes\n",
            "checking if g++ supports stl <iostream> (required)... yes\n",
            "checking if g++ supports stl <sstream> (required)... yes\n",
            "checking if g++ supports stl <fstream> (required)... yes\n",
            "checking if g++ supports template <class T> (required)... yes\n",
            "checking if g++ supports const_cast<> (required)... yes\n",
            "checking if g++ supports static_cast<> (required)... yes\n",
            "checking if g++ supports reinterpret_cast<> (required)... yes\n",
            "checking if g++ supports namespaces (required) ... yes\n",
            "checking if g++ supports __thread (optional)... yes\n",
            "checking if g++ supports template <class T> (required)... yes\n",
            "checking if g++ supports GCC native atomic operations (optional)... yes\n",
            "checking if g++ supports OSX native atomic operations (optional)... no\n",
            "checking if g++ environment provides all required features... yes\n",
            "configure: creating ./config.status\n",
            "config.status: creating Makefile\n",
            "config.status: creating src/Makefile\n",
            "config.status: creating src/Makefile.msvc\n",
            "config.status: creating man/Makefile\n",
            "config.status: creating doc/Makefile\n",
            "config.status: creating tests/Makefile\n",
            "config.status: creating swig/version.h\n",
            "config.status: creating mecab.iss\n",
            "config.status: creating mecab-config\n",
            "config.status: creating mecabrc\n",
            "config.status: creating config.h\n",
            "config.status: executing depfiles commands\n",
            "config.status: executing libtool commands\n",
            "config.status: executing default commands\n",
            "+ make\n",
            "make  all-recursive\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "Making all in src\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o viterbi.lo viterbi.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c viterbi.cpp  -fPIC -DPIC -o .libs/viterbi.o\n",
            "In file included from \u001b[01m\u001b[Kviterbi.cpp:14:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kparam.h:30:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K'\u001b[01m\u001b[KTarget {anonymous}::lexical_cast(Source) [with Target = std::__cxx11::basic_string<char>; Source = std::__cxx11::basic_string<char>]\u001b[m\u001b[K' defined but not used [\u001b[01;35m\u001b[K-Wunused-function\u001b[m\u001b[K]\n",
            " std::string \u001b[01;35m\u001b[Klexical_cast<std::string, std::string>\u001b[m\u001b[K(std::string arg) {\n",
            "             \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c viterbi.cpp -o viterbi.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o tagger.lo tagger.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c tagger.cpp  -fPIC -DPIC -o .libs/tagger.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c tagger.cpp -o tagger.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o utils.lo utils.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c utils.cpp  -fPIC -DPIC -o .libs/utils.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c utils.cpp -o utils.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o eval.lo eval.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c eval.cpp  -fPIC -DPIC -o .libs/eval.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c eval.cpp -o eval.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o iconv_utils.lo iconv_utils.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c iconv_utils.cpp  -fPIC -DPIC -o .libs/iconv_utils.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c iconv_utils.cpp -o iconv_utils.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o dictionary_rewriter.lo dictionary_rewriter.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary_rewriter.cpp  -fPIC -DPIC -o .libs/dictionary_rewriter.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary_rewriter.cpp -o dictionary_rewriter.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o dictionary_generator.lo dictionary_generator.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary_generator.cpp  -fPIC -DPIC -o .libs/dictionary_generator.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary_generator.cpp -o dictionary_generator.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o dictionary_compiler.lo dictionary_compiler.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary_compiler.cpp  -fPIC -DPIC -o .libs/dictionary_compiler.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary_compiler.cpp -o dictionary_compiler.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o context_id.lo context_id.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c context_id.cpp  -fPIC -DPIC -o .libs/context_id.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c context_id.cpp -o context_id.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o connector.lo connector.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c connector.cpp  -fPIC -DPIC -o .libs/connector.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c connector.cpp -o connector.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o nbest_generator.lo nbest_generator.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c nbest_generator.cpp  -fPIC -DPIC -o .libs/nbest_generator.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c nbest_generator.cpp -o nbest_generator.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o writer.lo writer.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c writer.cpp  -fPIC -DPIC -o .libs/writer.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c writer.cpp -o writer.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o string_buffer.lo string_buffer.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c string_buffer.cpp  -fPIC -DPIC -o .libs/string_buffer.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c string_buffer.cpp -o string_buffer.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o param.lo param.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c param.cpp  -fPIC -DPIC -o .libs/param.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c param.cpp -o param.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o tokenizer.lo tokenizer.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c tokenizer.cpp  -fPIC -DPIC -o .libs/tokenizer.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c tokenizer.cpp -o tokenizer.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o char_property.lo char_property.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c char_property.cpp  -fPIC -DPIC -o .libs/char_property.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c char_property.cpp -o char_property.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o dictionary.lo dictionary.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary.cpp  -fPIC -DPIC -o .libs/dictionary.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary.cpp -o dictionary.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o feature_index.lo feature_index.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c feature_index.cpp  -fPIC -DPIC -o .libs/feature_index.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c feature_index.cpp -o feature_index.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o lbfgs.lo lbfgs.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c lbfgs.cpp  -fPIC -DPIC -o .libs/lbfgs.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c lbfgs.cpp -o lbfgs.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o learner_tagger.lo learner_tagger.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c learner_tagger.cpp  -fPIC -DPIC -o .libs/learner_tagger.o\n",
            "\u001b[01m\u001b[Klearner_tagger.cpp:25:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K'\u001b[01m\u001b[Kchar* MeCab::{anonymous}::mystrdup(const string&)\u001b[m\u001b[K' defined but not used [\u001b[01;35m\u001b[K-Wunused-function\u001b[m\u001b[K]\n",
            " char *\u001b[01;35m\u001b[Kmystrdup\u001b[m\u001b[K(const std::string &str) {\n",
            "       \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c learner_tagger.cpp -o learner_tagger.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o learner.lo learner.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c learner.cpp  -fPIC -DPIC -o .libs/learner.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c learner.cpp -o learner.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o libmecab.lo libmecab.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c libmecab.cpp  -fPIC -DPIC -o .libs/libmecab.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c libmecab.cpp -o libmecab.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall  -no-undefined -version-info 2:0:0  -o libmecab.la -rpath /usr/local/lib viterbi.lo tagger.lo utils.lo eval.lo iconv_utils.lo dictionary_rewriter.lo dictionary_generator.lo dictionary_compiler.lo context_id.lo connector.lo nbest_generator.lo writer.lo string_buffer.lo param.lo tokenizer.lo char_property.lo dictionary.lo feature_index.lo lbfgs.lo learner_tagger.lo learner.lo libmecab.lo  -lpthread -lpthread  -lstdc++ \n",
            "libtool: link: g++  -fPIC -DPIC -shared -nostdlib /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/7/crtbeginS.o  .libs/viterbi.o .libs/tagger.o .libs/utils.o .libs/eval.o .libs/iconv_utils.o .libs/dictionary_rewriter.o .libs/dictionary_generator.o .libs/dictionary_compiler.o .libs/context_id.o .libs/connector.o .libs/nbest_generator.o .libs/writer.o .libs/string_buffer.o .libs/param.o .libs/tokenizer.o .libs/char_property.o .libs/dictionary.o .libs/feature_index.o .libs/lbfgs.o .libs/learner_tagger.o .libs/learner.o .libs/libmecab.o   -lpthread -L/usr/lib/gcc/x86_64-linux-gnu/7 -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../../lib -L/lib/x86_64-linux-gnu -L/lib/../lib -L/usr/lib/x86_64-linux-gnu -L/usr/lib/../lib -L/usr/local/cuda/lib64/stubs -L/usr/lib/gcc/x86_64-linux-gnu/7/../../.. -lstdc++ -lm -lc -lgcc_s /usr/lib/gcc/x86_64-linux-gnu/7/crtendS.o /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crtn.o  -O3   -Wl,-soname -Wl,libmecab.so.2 -o .libs/libmecab.so.2.0.0\n",
            "libtool: link: (cd \".libs\" && rm -f \"libmecab.so.2\" && ln -s \"libmecab.so.2.0.0\" \"libmecab.so.2\")\n",
            "libtool: link: (cd \".libs\" && rm -f \"libmecab.so\" && ln -s \"libmecab.so.2.0.0\" \"libmecab.so\")\n",
            "libtool: link: ar cru .libs/libmecab.a  viterbi.o tagger.o utils.o eval.o iconv_utils.o dictionary_rewriter.o dictionary_generator.o dictionary_compiler.o context_id.o connector.o nbest_generator.o writer.o string_buffer.o param.o tokenizer.o char_property.o dictionary.o feature_index.o lbfgs.o learner_tagger.o learner.o libmecab.o\n",
            "ar: `u' modifier ignored since `D' is the default (see `U')\n",
            "libtool: link: ranlib .libs/libmecab.a\n",
            "libtool: link: ( cd \".libs\" && rm -f \"libmecab.la\" && ln -s \"../libmecab.la\" \"libmecab.la\" )\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o mecab.o mecab.cpp\n",
            "/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall    -o mecab mecab.o libmecab.la -lpthread -lpthread  -lstdc++ \n",
            "libtool: link: g++ -O3 -Wall -o .libs/mecab mecab.o  ./.libs/libmecab.so -lpthread -lstdc++\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o mecab-dict-index.o mecab-dict-index.cpp\n",
            "/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall    -o mecab-dict-index mecab-dict-index.o libmecab.la -lpthread -lpthread  -lstdc++ \n",
            "libtool: link: g++ -O3 -Wall -o .libs/mecab-dict-index mecab-dict-index.o  ./.libs/libmecab.so -lpthread -lstdc++\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o mecab-dict-gen.o mecab-dict-gen.cpp\n",
            "/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall    -o mecab-dict-gen mecab-dict-gen.o libmecab.la -lpthread -lpthread  -lstdc++ \n",
            "libtool: link: g++ -O3 -Wall -o .libs/mecab-dict-gen mecab-dict-gen.o  ./.libs/libmecab.so -lpthread -lstdc++\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o mecab-cost-train.o mecab-cost-train.cpp\n",
            "/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall    -o mecab-cost-train mecab-cost-train.o libmecab.la -lpthread -lpthread  -lstdc++ \n",
            "libtool: link: g++ -O3 -Wall -o .libs/mecab-cost-train mecab-cost-train.o  ./.libs/libmecab.so -lpthread -lstdc++\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o mecab-system-eval.o mecab-system-eval.cpp\n",
            "/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall    -o mecab-system-eval mecab-system-eval.o libmecab.la -lpthread -lpthread  -lstdc++ \n",
            "libtool: link: g++ -O3 -Wall -o .libs/mecab-system-eval mecab-system-eval.o  ./.libs/libmecab.so -lpthread -lstdc++\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o mecab-test-gen.o mecab-test-gen.cpp\n",
            "/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall    -o mecab-test-gen mecab-test-gen.o libmecab.la -lpthread -lpthread  -lstdc++ \n",
            "libtool: link: g++ -O3 -Wall -o .libs/mecab-test-gen mecab-test-gen.o  ./.libs/libmecab.so -lpthread -lstdc++\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
            "Making all in man\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
            "make[2]: Nothing to be done for 'all'.\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
            "Making all in doc\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
            "make[2]: Nothing to be done for 'all'.\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
            "Making all in tests\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "make[2]: Nothing to be done for 'all'.\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "+ make check\n",
            "Making check in src\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
            "make[1]: Nothing to be done for 'check'.\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
            "Making check in man\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
            "make[1]: Nothing to be done for 'check'.\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
            "Making check in doc\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
            "make[1]: Nothing to be done for 'check'.\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
            "Making check in tests\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "make  check-TESTS\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./unk.def ... 2\n",
            "emitting double-array: 100% |###########################################| \n",
            "./model.def is not found. skipped.\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./dic.csv ... 177\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading ./matrix.def ... 178x178\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./unk.def ... 2\n",
            "emitting double-array: 100% |###########################################| \n",
            "./model.def is not found. skipped.\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./dic.csv ... 83\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading ./matrix.def ... 84x84\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./unk.def ... 2\n",
            "emitting double-array: 100% |###########################################| \n",
            "./model.def is not found. skipped.\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./dic.csv ... 450\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading ./matrix.def ... 1x1\n",
            "\n",
            "done!\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./unk.def ... 2\n",
            "emitting double-array: 100% |###########################################| \n",
            "./model.def is not found. skipped.\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./dic.csv ... 162\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading ./matrix.def ... 3x3\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./unk.def ... 2\n",
            "emitting double-array: 100% |###########################################| \n",
            "./model.def is not found. skipped.\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./dic.csv ... 4\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading ./matrix.def ... 1x1\n",
            "\n",
            "done!\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./unk.def ... 11\n",
            "emitting double-array: 100% |###########################################| \n",
            "./model.def is not found. skipped.\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./dic.csv ... 1\n",
            "reading ./matrix.def ... 1x1\n",
            "\n",
            "done!\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./unk.def ... 2\n",
            "emitting double-array: 100% |###########################################| \n",
            "./model.def is not found. skipped.\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./dic.csv ... 1\n",
            "reading ./matrix.def ... 1x1\n",
            "\n",
            "done!\n",
            "PASS: run-dics.sh\n",
            "PASS: run-eval.sh\n",
            "seed/pos-id.def is not found. minimum setting is used\n",
            "reading seed/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "seed/model.def is not found. skipped.\n",
            "seed/pos-id.def is not found. minimum setting is used\n",
            "reading seed/dic.csv ... 4335\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading seed/matrix.def ... 1x1\n",
            "\n",
            "done!\n",
            "reading corpus ...\n",
            "Number of sentences: 34\n",
            "Number of features:  64108\n",
            "eta:                 0.00005\n",
            "freq:                1\n",
            "eval-size:           6\n",
            "unk-eval-size:       4\n",
            "threads:             1\n",
            "charset:             EUC-JP\n",
            "C(sigma^2):          1.00000\n",
            "\n",
            "iter=0 err=1.00000 F=0.35771 target=2406.28355 diff=1.00000\n",
            "iter=1 err=0.97059 F=0.65652 target=1484.25231 diff=0.38318\n",
            "iter=2 err=0.91176 F=0.79331 target=863.32765 diff=0.41834\n",
            "iter=3 err=0.85294 F=0.89213 target=596.72480 diff=0.30881\n",
            "iter=4 err=0.61765 F=0.95467 target=336.30744 diff=0.43641\n",
            "iter=5 err=0.50000 F=0.96702 target=246.53039 diff=0.26695\n",
            "iter=6 err=0.35294 F=0.95472 target=188.93963 diff=0.23361\n",
            "iter=7 err=0.20588 F=0.99106 target=168.62665 diff=0.10751\n",
            "iter=8 err=0.05882 F=0.99777 target=158.64865 diff=0.05917\n",
            "iter=9 err=0.08824 F=0.99665 target=154.14530 diff=0.02839\n",
            "iter=10 err=0.08824 F=0.99665 target=151.94257 diff=0.01429\n",
            "iter=11 err=0.02941 F=0.99888 target=147.20825 diff=0.03116\n",
            "iter=12 err=0.00000 F=1.00000 target=147.34956 diff=0.00096\n",
            "iter=13 err=0.02941 F=0.99888 target=146.32592 diff=0.00695\n",
            "iter=14 err=0.00000 F=1.00000 target=145.77299 diff=0.00378\n",
            "iter=15 err=0.02941 F=0.99888 target=145.24641 diff=0.00361\n",
            "iter=16 err=0.00000 F=1.00000 target=144.96490 diff=0.00194\n",
            "iter=17 err=0.02941 F=0.99888 target=144.90246 diff=0.00043\n",
            "iter=18 err=0.00000 F=1.00000 target=144.75959 diff=0.00099\n",
            "iter=19 err=0.00000 F=1.00000 target=144.71727 diff=0.00029\n",
            "iter=20 err=0.00000 F=1.00000 target=144.66337 diff=0.00037\n",
            "iter=21 err=0.00000 F=1.00000 target=144.61349 diff=0.00034\n",
            "iter=22 err=0.00000 F=1.00000 target=144.62987 diff=0.00011\n",
            "iter=23 err=0.00000 F=1.00000 target=144.60060 diff=0.00020\n",
            "iter=24 err=0.00000 F=1.00000 target=144.59125 diff=0.00006\n",
            "iter=25 err=0.00000 F=1.00000 target=144.58619 diff=0.00004\n",
            "iter=26 err=0.00000 F=1.00000 target=144.58219 diff=0.00003\n",
            "iter=27 err=0.00000 F=1.00000 target=144.58059 diff=0.00001\n",
            "\n",
            "Done! writing model file ... \n",
            "model-ipadic.c1.0.f1.model is not a binary model. reopen it as text mode...\n",
            "reading seed/unk.def ... 40\n",
            "reading seed/dic.csv ... 4335\n",
            "emitting model-ipadic.c1.0.f1.dic/left-id.def/ model-ipadic.c1.0.f1.dic/right-id.def\n",
            "emitting model-ipadic.c1.0.f1.dic/unk.def ... 40\n",
            "emitting model-ipadic.c1.0.f1.dic/dic.csv ... 4335\n",
            "emitting matrix      : 100% |###########################################| \n",
            "copying seed/char.def to model-ipadic.c1.0.f1.dic/char.def\n",
            "copying seed/rewrite.def to model-ipadic.c1.0.f1.dic/rewrite.def\n",
            "copying seed/dicrc to model-ipadic.c1.0.f1.dic/dicrc\n",
            "copying seed/feature.def to model-ipadic.c1.0.f1.dic/feature.def\n",
            "copying model-ipadic.c1.0.f1.model to model-ipadic.c1.0.f1.dic/model.def\n",
            "\n",
            "done!\n",
            "model-ipadic.c1.0.f1.dic/pos-id.def is not found. minimum setting is used\n",
            "reading model-ipadic.c1.0.f1.dic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "model-ipadic.c1.0.f1.dic/pos-id.def is not found. minimum setting is used\n",
            "reading model-ipadic.c1.0.f1.dic/dic.csv ... 4335\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading model-ipadic.c1.0.f1.dic/matrix.def ... 346x346\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "              precision          recall         F\n",
            "LEVEL 0:    12.8959(57/442) 11.8998(57/479) 12.3779\n",
            "LEVEL 1:    12.2172(54/442) 11.2735(54/479) 11.7264\n",
            "LEVEL 2:    11.7647(52/442) 10.8559(52/479) 11.2921\n",
            "LEVEL 4:    11.7647(52/442) 10.8559(52/479) 11.2921\n",
            "PASS: run-cost-train.sh\n",
            "==================\n",
            "All 3 tests passed\n",
            "==================\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "+ sudo make install\n",
            "Making install in src\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
            "test -z \"/usr/local/lib\" || /bin/mkdir -p \"/usr/local/lib\"\n",
            " /bin/bash ../libtool   --mode=install /usr/bin/install -c   libmecab.la '/usr/local/lib'\n",
            "libtool: install: /usr/bin/install -c .libs/libmecab.so.2.0.0 /usr/local/lib/libmecab.so.2.0.0\n",
            "libtool: install: (cd /usr/local/lib && { ln -s -f libmecab.so.2.0.0 libmecab.so.2 || { rm -f libmecab.so.2 && ln -s libmecab.so.2.0.0 libmecab.so.2; }; })\n",
            "libtool: install: (cd /usr/local/lib && { ln -s -f libmecab.so.2.0.0 libmecab.so || { rm -f libmecab.so && ln -s libmecab.so.2.0.0 libmecab.so; }; })\n",
            "libtool: install: /usr/bin/install -c .libs/libmecab.lai /usr/local/lib/libmecab.la\n",
            "libtool: install: /usr/bin/install -c .libs/libmecab.a /usr/local/lib/libmecab.a\n",
            "libtool: install: chmod 644 /usr/local/lib/libmecab.a\n",
            "libtool: install: ranlib /usr/local/lib/libmecab.a\n",
            "libtool: finish: PATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin:/sbin\" ldconfig -n /usr/local/lib\n",
            "----------------------------------------------------------------------\n",
            "Libraries have been installed in:\n",
            "   /usr/local/lib\n",
            "\n",
            "If you ever happen to want to link against installed libraries\n",
            "in a given directory, LIBDIR, you must either use libtool, and\n",
            "specify the full pathname of the library, or use the `-LLIBDIR'\n",
            "flag during linking and do at least one of the following:\n",
            "   - add LIBDIR to the `LD_LIBRARY_PATH' environment variable\n",
            "     during execution\n",
            "   - add LIBDIR to the `LD_RUN_PATH' environment variable\n",
            "     during linking\n",
            "   - use the `-Wl,-rpath -Wl,LIBDIR' linker flag\n",
            "   - have your system administrator add LIBDIR to `/etc/ld.so.conf'\n",
            "\n",
            "See any operating system documentation about shared libraries for\n",
            "more information, such as the ld(1) and ld.so(8) manual pages.\n",
            "----------------------------------------------------------------------\n",
            "test -z \"/usr/local/bin\" || /bin/mkdir -p \"/usr/local/bin\"\n",
            "  /bin/bash ../libtool   --mode=install /usr/bin/install -c mecab '/usr/local/bin'\n",
            "libtool: install: /usr/bin/install -c .libs/mecab /usr/local/bin/mecab\n",
            "test -z \"/usr/local/libexec/mecab\" || /bin/mkdir -p \"/usr/local/libexec/mecab\"\n",
            "  /bin/bash ../libtool   --mode=install /usr/bin/install -c mecab-dict-index mecab-dict-gen mecab-cost-train mecab-system-eval mecab-test-gen '/usr/local/libexec/mecab'\n",
            "libtool: install: /usr/bin/install -c .libs/mecab-dict-index /usr/local/libexec/mecab/mecab-dict-index\n",
            "libtool: install: /usr/bin/install -c .libs/mecab-dict-gen /usr/local/libexec/mecab/mecab-dict-gen\n",
            "libtool: install: /usr/bin/install -c .libs/mecab-cost-train /usr/local/libexec/mecab/mecab-cost-train\n",
            "libtool: install: /usr/bin/install -c .libs/mecab-system-eval /usr/local/libexec/mecab/mecab-system-eval\n",
            "libtool: install: /usr/bin/install -c .libs/mecab-test-gen /usr/local/libexec/mecab/mecab-test-gen\n",
            "test -z \"/usr/local/include\" || /bin/mkdir -p \"/usr/local/include\"\n",
            " /usr/bin/install -c -m 644 mecab.h '/usr/local/include'\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
            "Making install in man\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
            "make[2]: Nothing to be done for 'install-exec-am'.\n",
            "test -z \"/usr/local/share/man/man1\" || /bin/mkdir -p \"/usr/local/share/man/man1\"\n",
            " /usr/bin/install -c -m 644 mecab.1 '/usr/local/share/man/man1'\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
            "Making install in doc\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
            "make[2]: Nothing to be done for 'install-exec-am'.\n",
            "make[2]: Nothing to be done for 'install-data-am'.\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
            "Making install in tests\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "make[2]: Nothing to be done for 'install-exec-am'.\n",
            "make[2]: Nothing to be done for 'install-data-am'.\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "test -z \"/usr/local/bin\" || /bin/mkdir -p \"/usr/local/bin\"\n",
            " /usr/bin/install -c mecab-config '/usr/local/bin'\n",
            "test -z \"/usr/local/etc\" || /bin/mkdir -p \"/usr/local/etc\"\n",
            " /usr/bin/install -c -m 644 mecabrc '/usr/local/etc'\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "+ [[ -d /usr/local/lib/mecab/dic/mecab-ko-dic ]]\n",
            "+ echo 'Install mecab-ko-dic'\n",
            "Install mecab-ko-dic\n",
            "+ install_mecab_ko_dic\n",
            "+ echo 'Install mecab-ko-dic'\n",
            "Install mecab-ko-dic\n",
            "+ cd /tmp\n",
            "+ curl -LO https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 47.4M  100 47.4M    0     0  35.5M      0  0:00:01  0:00:01 --:--:-- 86.7M\n",
            "+ tar -zxvf mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "mecab-ko-dic-2.1.1-20180720/\n",
            "mecab-ko-dic-2.1.1-20180720/configure\n",
            "mecab-ko-dic-2.1.1-20180720/COPYING\n",
            "mecab-ko-dic-2.1.1-20180720/autogen.sh\n",
            "mecab-ko-dic-2.1.1-20180720/Place-station.csv\n",
            "mecab-ko-dic-2.1.1-20180720/NNG.csv\n",
            "mecab-ko-dic-2.1.1-20180720/README\n",
            "mecab-ko-dic-2.1.1-20180720/EF.csv\n",
            "mecab-ko-dic-2.1.1-20180720/MAG.csv\n",
            "mecab-ko-dic-2.1.1-20180720/Preanalysis.csv\n",
            "mecab-ko-dic-2.1.1-20180720/NNB.csv\n",
            "mecab-ko-dic-2.1.1-20180720/Person-actor.csv\n",
            "mecab-ko-dic-2.1.1-20180720/VV.csv\n",
            "mecab-ko-dic-2.1.1-20180720/Makefile.in\n",
            "mecab-ko-dic-2.1.1-20180720/matrix.def\n",
            "mecab-ko-dic-2.1.1-20180720/EC.csv\n",
            "mecab-ko-dic-2.1.1-20180720/NNBC.csv\n",
            "mecab-ko-dic-2.1.1-20180720/clean\n",
            "mecab-ko-dic-2.1.1-20180720/ChangeLog\n",
            "mecab-ko-dic-2.1.1-20180720/J.csv\n",
            "mecab-ko-dic-2.1.1-20180720/.keep\n",
            "mecab-ko-dic-2.1.1-20180720/feature.def\n",
            "mecab-ko-dic-2.1.1-20180720/Foreign.csv\n",
            "mecab-ko-dic-2.1.1-20180720/XPN.csv\n",
            "mecab-ko-dic-2.1.1-20180720/EP.csv\n",
            "mecab-ko-dic-2.1.1-20180720/NR.csv\n",
            "mecab-ko-dic-2.1.1-20180720/left-id.def\n",
            "mecab-ko-dic-2.1.1-20180720/Place.csv\n",
            "mecab-ko-dic-2.1.1-20180720/Symbol.csv\n",
            "mecab-ko-dic-2.1.1-20180720/dicrc\n",
            "mecab-ko-dic-2.1.1-20180720/NP.csv\n",
            "mecab-ko-dic-2.1.1-20180720/ETM.csv\n",
            "mecab-ko-dic-2.1.1-20180720/IC.csv\n",
            "mecab-ko-dic-2.1.1-20180720/Place-address.csv\n",
            "mecab-ko-dic-2.1.1-20180720/Group.csv\n",
            "mecab-ko-dic-2.1.1-20180720/model.def\n",
            "mecab-ko-dic-2.1.1-20180720/XSN.csv\n",
            "mecab-ko-dic-2.1.1-20180720/INSTALL\n",
            "mecab-ko-dic-2.1.1-20180720/rewrite.def\n",
            "mecab-ko-dic-2.1.1-20180720/Inflect.csv\n",
            "mecab-ko-dic-2.1.1-20180720/configure.ac\n",
            "mecab-ko-dic-2.1.1-20180720/NNP.csv\n",
            "mecab-ko-dic-2.1.1-20180720/CoinedWord.csv\n",
            "mecab-ko-dic-2.1.1-20180720/XSV.csv\n",
            "mecab-ko-dic-2.1.1-20180720/pos-id.def\n",
            "mecab-ko-dic-2.1.1-20180720/Makefile.am\n",
            "mecab-ko-dic-2.1.1-20180720/unk.def\n",
            "mecab-ko-dic-2.1.1-20180720/missing\n",
            "mecab-ko-dic-2.1.1-20180720/VCP.csv\n",
            "mecab-ko-dic-2.1.1-20180720/install-sh\n",
            "mecab-ko-dic-2.1.1-20180720/Hanja.csv\n",
            "mecab-ko-dic-2.1.1-20180720/MAJ.csv\n",
            "mecab-ko-dic-2.1.1-20180720/XSA.csv\n",
            "mecab-ko-dic-2.1.1-20180720/Wikipedia.csv\n",
            "mecab-ko-dic-2.1.1-20180720/tools/\n",
            "mecab-ko-dic-2.1.1-20180720/tools/add-userdic.sh\n",
            "mecab-ko-dic-2.1.1-20180720/tools/mecab-bestn.sh\n",
            "mecab-ko-dic-2.1.1-20180720/tools/convert_for_using_store.sh\n",
            "mecab-ko-dic-2.1.1-20180720/user-dic/\n",
            "mecab-ko-dic-2.1.1-20180720/user-dic/nnp.csv\n",
            "mecab-ko-dic-2.1.1-20180720/user-dic/place.csv\n",
            "mecab-ko-dic-2.1.1-20180720/user-dic/person.csv\n",
            "mecab-ko-dic-2.1.1-20180720/user-dic/README.md\n",
            "mecab-ko-dic-2.1.1-20180720/NorthKorea.csv\n",
            "mecab-ko-dic-2.1.1-20180720/VX.csv\n",
            "mecab-ko-dic-2.1.1-20180720/right-id.def\n",
            "mecab-ko-dic-2.1.1-20180720/VA.csv\n",
            "mecab-ko-dic-2.1.1-20180720/char.def\n",
            "mecab-ko-dic-2.1.1-20180720/NEWS\n",
            "mecab-ko-dic-2.1.1-20180720/MM.csv\n",
            "mecab-ko-dic-2.1.1-20180720/ETN.csv\n",
            "mecab-ko-dic-2.1.1-20180720/AUTHORS\n",
            "mecab-ko-dic-2.1.1-20180720/Person.csv\n",
            "mecab-ko-dic-2.1.1-20180720/XR.csv\n",
            "mecab-ko-dic-2.1.1-20180720/VCN.csv\n",
            "+ cd mecab-ko-dic-2.1.1-20180720\n",
            "+ ./autogen.sh\n",
            "Looking in current directory for macros.\n",
            "configure.ac:2: warning: AM_INIT_AUTOMAKE: two- and three-arguments forms are deprecated.  For more info, see:\n",
            "configure.ac:2: http://www.gnu.org/software/automake/manual/automake.html#Modernize-AM_005fINIT_005fAUTOMAKE-invocation\n",
            "+ ./configure\n",
            "checking for a BSD-compatible install... /usr/bin/install -c\n",
            "checking whether build environment is sane... yes\n",
            "/tmp/mecab-ko-dic-2.1.1-20180720/missing: Unknown `--is-lightweight' option\n",
            "Try `/tmp/mecab-ko-dic-2.1.1-20180720/missing --help' for more information\n",
            "configure: WARNING: 'missing' script is too old or missing\n",
            "checking for a thread-safe mkdir -p... /bin/mkdir -p\n",
            "checking for gawk... no\n",
            "checking for mawk... mawk\n",
            "checking whether make sets $(MAKE)... yes\n",
            "checking whether make supports nested variables... yes\n",
            "checking for mecab-config... /usr/local/bin/mecab-config\n",
            "checking that generated files are newer than configure... done\n",
            "configure: creating ./config.status\n",
            "config.status: creating Makefile\n",
            "+ [[ Linux == \\L\\i\\n\\u\\x ]]\n",
            "+ sudo tee /etc/ld.so.conf.d/mecab.conf\n",
            "+ mecab-config --libs-only-L\n",
            "/usr/local/lib\n",
            "+ sudo ldconfig\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "+ make\n",
            "/usr/local/libexec/mecab/mecab-dict-index -d . -o . -f UTF-8 -t UTF-8\n",
            "reading ./unk.def ... 13\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading ./Group.csv ... 3176\n",
            "reading ./NR.csv ... 482\n",
            "reading ./MAG.csv ... 14242\n",
            "reading ./Foreign.csv ... 11690\n",
            "reading ./XPN.csv ... 83\n",
            "reading ./XSV.csv ... 23\n",
            "reading ./EC.csv ... 2547\n",
            "reading ./NNB.csv ... 140\n",
            "reading ./VA.csv ... 2360\n",
            "reading ./Place-address.csv ... 19301\n",
            "reading ./EF.csv ... 1820\n",
            "reading ./Inflect.csv ... 44820\n",
            "reading ./VV.csv ... 7331\n",
            "reading ./Place-station.csv ... 1145\n",
            "reading ./NNG.csv ... 208524\n",
            "reading ./NNBC.csv ... 677\n",
            "reading ./J.csv ... 416\n",
            "reading ./Wikipedia.csv ... 36762\n",
            "reading ./MAJ.csv ... 240\n",
            "reading ./VCN.csv ... 7\n",
            "reading ./CoinedWord.csv ... 148\n",
            "reading ./VCP.csv ... 9\n",
            "reading ./XSN.csv ... 124\n",
            "reading ./ETN.csv ... 14\n",
            "reading ./VX.csv ... 125\n",
            "reading ./XSA.csv ... 19\n",
            "reading ./EP.csv ... 51\n",
            "reading ./NorthKorea.csv ... 3\n",
            "reading ./Symbol.csv ... 16\n",
            "reading ./XR.csv ... 3637\n",
            "reading ./NNP.csv ... 2371\n",
            "reading ./Person-actor.csv ... 99230\n",
            "reading ./IC.csv ... 1305\n",
            "reading ./ETM.csv ... 133\n",
            "reading ./MM.csv ... 453\n",
            "reading ./Place.csv ... 30303\n",
            "reading ./Preanalysis.csv ... 5\n",
            "reading ./Person.csv ... 196459\n",
            "reading ./NP.csv ... 342\n",
            "reading ./Hanja.csv ... 125750\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading ./matrix.def ... 3822x2693\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "echo To enable dictionary, rewrite /usr/local/etc/mecabrc as \\\"dicdir = /usr/local/lib/mecab/dic/mecab-ko-dic\\\"\n",
            "To enable dictionary, rewrite /usr/local/etc/mecabrc as \"dicdir = /usr/local/lib/mecab/dic/mecab-ko-dic\"\n",
            "+ sudo sh -c 'echo \"dicdir=/usr/local/lib/mecab/dic/mecab-ko-dic\" > /usr/local/etc/mecabrc'\n",
            "+ sudo make install\n",
            "make[1]: Entering directory '/tmp/mecab-ko-dic-2.1.1-20180720'\n",
            "make[1]: Nothing to be done for 'install-exec-am'.\n",
            " /bin/mkdir -p '/usr/local/lib/mecab/dic/mecab-ko-dic'\n",
            " /usr/bin/install -c -m 644 model.bin matrix.bin char.bin sys.dic unk.dic left-id.def right-id.def rewrite.def pos-id.def dicrc '/usr/local/lib/mecab/dic/mecab-ko-dic'\n",
            "make[1]: Leaving directory '/tmp/mecab-ko-dic-2.1.1-20180720'\n",
            "++ python3 -c 'import pkgutil; print(1 if pkgutil.find_loader(\"MeCab\") else 0)'\n",
            "+ [[ 0 == \\1 ]]\n",
            "+ echo 'Install mecab-python'\n",
            "Install mecab-python\n",
            "+ install_mecab_python\n",
            "+ pushd /tmp\n",
            "/tmp /tmp/mecab-ko-dic-2.1.1-20180720\n",
            "+ [[ ! -d mecab-python-0.996 ]]\n",
            "+ git clone https://bitbucket.org/eunjeon/mecab-python-0.996.git\n",
            "Cloning into 'mecab-python-0.996'...\n",
            "Unpacking objects: 100% (17/17), done.\n",
            "+ popd\n",
            "/tmp/mecab-ko-dic-2.1.1-20180720\n",
            "+ python3 -m pip install /tmp/mecab-python-0.996\n",
            "Processing /tmp/mecab-python-0.996\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: mecab-python\n",
            "  Building wheel for mecab-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mecab-python: filename=mecab_python-0.996_ko_0.9.2-cp37-cp37m-linux_x86_64.whl size=141811 sha256=261188d55e8d9429f8b60e762f9bda416488f05b88561a7fc3a961769fa14a1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/7b/9f/2922869bef86c3354ae7034f7a3647c573ee1997c2dad0290a\n",
            "\u001b[33m  WARNING: Built wheel for mecab-python is invalid: Metadata 1.2 mandates PEP 440 version, but '0.996-ko-0.9.2' is not\u001b[0m\n",
            "Failed to build mecab-python\n",
            "Installing collected packages: mecab-python\n",
            "    Running setup.py install for mecab-python ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: mecab-python was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368.\u001b[0m\n",
            "Successfully installed mecab-python-0.996-ko-0.9.2\n",
            "+ echo Done.\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4WC7ksAPnot"
      },
      "source": [
        "### 단어 토큰화\n",
        "* 한국어는 공백으로 단어를 분리해도 조사, 접속사 등이 남아 분석에 어려움이 있음\n",
        "* 이를 해결해주는 한국어 토큰화는 조사, 접속사를 분리해주거나 제거\n",
        "* 한국어 토큰화를 사용하기 위해서는 konlpy와 mecab이라는 라이브러리가 필요\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_m-dXTPPiaW"
      },
      "source": [
        "from konlpy.tag import Mecab\n",
        "tagger = Mecab() #Mecab이 제일 성능이 좋음"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHuoyZ6RQLqa",
        "outputId": "97f2c852-190e-478f-c137-c2ffe625e974"
      },
      "source": [
        "sentence = \"언제나 현재에 집중할 수 있다면 행복할것이다.\"\n",
        "tagger.pos(sentence)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('언제나', 'MAG'),\n",
              " ('현재', 'NNG'),\n",
              " ('에', 'JKB'),\n",
              " ('집중', 'NNG'),\n",
              " ('할', 'XSV+ETM'),\n",
              " ('수', 'NNB'),\n",
              " ('있', 'VV'),\n",
              " ('다면', 'EC'),\n",
              " ('행복', 'NNG'),\n",
              " ('할', 'XSV+ETM'),\n",
              " ('것', 'NNB'),\n",
              " ('이', 'VCP'),\n",
              " ('다', 'EF'),\n",
              " ('.', 'SF')]"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm-wwNjUQXNx"
      },
      "source": [
        "* 토큰화만 실행할 때는 tagger.morphs()라는 함수를 이용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kjhygt0QUd2",
        "outputId": "247a6639-43fb-4caa-a021-23b775d0900c"
      },
      "source": [
        "tagger.morphs(sentence)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['언제나', '현재', '에', '집중', '할', '수', '있', '다면', '행복', '할', '것', '이', '다', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBzw_z_BQlda"
      },
      "source": [
        "* 형태소만 사용하고 싶을 때는 tagger.nouns()라는 함수를 이용해 조사, 접속사 등을 제거 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hRxow-YQkPd",
        "outputId": "3cbffb91-d194-4410-9100-917c6d23d7f3"
      },
      "source": [
        "tagger.nouns(sentence)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['현재', '집중', '수', '행복', '것']"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt52CHpAQt0a"
      },
      "source": [
        "### 문장 토큰화\n",
        "* 한국어 문장을 토큰화할 때는 kss(korean sentence splitter) 라이브러리 이용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeagMuFxQsHs",
        "outputId": "3b04d8a0-6732-4603-c36c-7e8b24eb1cb8"
      },
      "source": [
        "!pip install kss"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kss\n",
            "  Downloading kss-3.2.0.tar.gz (42.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.4 MB 56 kB/s \n",
            "\u001b[?25hCollecting emoji\n",
            "  Downloading emoji-1.5.0.tar.gz (185 kB)\n",
            "\u001b[K     |████████████████████████████████| 185 kB 55.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kss, emoji\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-3.2.0-py3-none-any.whl size=42447995 sha256=91bcc71ba26570f39ffaa0e7b9851eac0e49ed12bf0913d9951c2eb8e228c160\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/47/9c/a5f83b5ab6096e3c4a33643fc553b26098c23e72b6539b86f4\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.5.0-py3-none-any.whl size=187457 sha256=c241a289d0c171fab010026b6386ea3a5a98c01681938c0076f57b9d73fb903e\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/b5/f6/b39abf14e94b3d6640613bbe630a66c10ccf7a12882d064fb5\n",
            "Successfully built kss emoji\n",
            "Installing collected packages: emoji, kss\n",
            "Successfully installed emoji-1.5.0 kss-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X66V1kWyRCj2"
      },
      "source": [
        "* 라이브러리를 이용해도 한국어에는 전치 표현이 존재해 제대로 토큰화가 안됨\n",
        "* 좀 더 나은 학습을 위해 사용자는 해당 부분을 따로 처리해주어야만 함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-EhFD6rQ-Ey",
        "outputId": "c08d40b2-37c2-4481-b1b2-20d97a5bab62"
      },
      "source": [
        "import kss\n",
        "text = '진짜? 내일 뭐하지. 이렇게 애매모호한 문장도? 밥은 먹었어? 나는...'\n",
        "print(kss.split_sentences(text))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['진짜? 내일 뭐하지.', '이렇게 애매모호한 문장도? 밥은 먹었어?', '나는...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBEywE7PRh6m"
      },
      "source": [
        "### 정규 표현식을 이용한 토큰화\n",
        "* 한국어도 정규 표현식을 이용해 토큰화 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co1fimzjRTw_",
        "outputId": "d9ea9a31-7bdc-4b16-e743-70646c3f4e83"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "sentence = '안녕하세요 ㅋㅋ 저는 자연어 처리(Natural Language Processing)를ㄹ!! 배우고 있습니다.'\n",
        "tokenizer = RegexpTokenizer(\"[가-힣]+\")\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['안녕하세요', '저는', '자연어', '처리', '를', '배우고', '있습니다']"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6avvmVaTR3vm",
        "outputId": "35dc910f-b7f0-451d-bfae-92ca0e3b4404"
      },
      "source": [
        "tokenizer = RegexpTokenizer(\"[ㄱ-ㅎ]+\", gaps=True) #자음을 기준으로\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['안녕하세요 ', ' 저는 자연어 처리(Natural Language Processing)를', '!! 배우고 있습니다.']"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc5-E7g3SR3W"
      },
      "source": [
        "### 케라스를 이용한 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMx1C6udSQYV",
        "outputId": "b1a8c234-733b-461c-cef3-9935b4c2a20f"
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "sentence = '성공의 비결은 단 한 가지, 잘할 수 있는 일에 광적으로 집중하는 것이다.'\n",
        "\n",
        "text_to_word_sequence(sentence) # 스페이스 기준으로 분류함"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['성공의', '비결은', '단', '한', '가지', '잘할', '수', '있는', '일에', '광적으로', '집중하는', '것이다']"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_Lx9tF8Sj6L"
      },
      "source": [
        "### TextBlob을 이용한 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVDUGkkjSe5h",
        "outputId": "2aa041b9-2f3e-418f-9bff-b9bfd91e77cb"
      },
      "source": [
        "from textblob import TextBlob\n",
        "blob = TextBlob(sentence)\n",
        "blob.words"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['성공의', '비결은', '단', '한', '가지', '잘할', '수', '있는', '일에', '광적으로', '집중하는', '것이다'])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pobOhkztSxKG"
      },
      "source": [
        "* 이 밖에도 한글의 토큰화 기법은 다양하나 이후에 알아보도록 하겠다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEiJIi4MS35V"
      },
      "source": [
        "### Bag of Words(BoW)\n",
        "* 단어들의 bag : 각 단어들이 몇개가 포함되어 있는지 단어들을 담는 가방으란 뜻이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-16Nyq6UOv1"
      },
      "source": [
        "* 영어"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u2NiQ5nS3H_",
        "outputId": "82be92e1-8050-4c00-bf5e-99c491fdae97"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\"Think like a man of action and act like man of thought\"]\n",
        "vector = CountVectorizer()\n",
        "bow = vector.fit_transform(corpus) #각 단어의 빈도수 저장\n",
        "\n",
        "print(bow.toarray())\n",
        "print(vector.vocabulary_)\n",
        "\n",
        "# 각 단어가 index로 들어가고 그 count가 값으로 들어감."
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 1 2 2 2 1 1]]\n",
            "{'think': 6, 'like': 3, 'man': 4, 'of': 5, 'action': 1, 'and': 2, 'act': 0, 'thought': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tU9XHmc2Svzo",
        "outputId": "d6836814-88cd-4839-dec5-6215eac2ee5a"
      },
      "source": [
        "# 불용어를 제외하도록 지정해줄 수 있다.\n",
        "vector = CountVectorizer(stop_words = 'english')\n",
        "bow = vector.fit_transform(corpus)\n",
        "print(bow.toarray())\n",
        "print(vector.vocabulary_)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 2 2 1 1]]\n",
            "{'think': 4, 'like': 2, 'man': 3, 'action': 1, 'act': 0, 'thought': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soe2nf7xUS_n"
      },
      "source": [
        "* 한국어"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bwa7eUySUI9n",
        "outputId": "b9fa6d1b-86ac-4549-de19-3e94dfd59ce5"
      },
      "source": [
        "courpus = [\"평생 살 것처럼 꿈을 꾸어라. 그리고 내일 죽을 것처럼 오늘을 살아라.\"]\n",
        "vector = CountVectorizer()\n",
        "bow=vector.fit_transform(courpus)\n",
        "print(bow.toarray())\n",
        "print(vector.vocabulary_)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2 1 1 1 1 1 1 1 1]]\n",
            "{'평생': 8, '것처럼': 0, '꿈을': 3, '꾸어라': 2, '그리고': 1, '내일': 4, '죽을': 7, '오늘을': 6, '살아라': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW3YEALWUjRl"
      },
      "source": [
        "* 한글은 위에 처럼 스페이스로 구분하는 것은 문제가 있음. 이를 보완하기 위해 정규화(regexpTokenizer) 와 형태소(Mecab)를 사용자가 지정해 줘야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seuUppTlUg91",
        "outputId": "4e0939ce-18e9-4eeb-f7fc-17c23c7e93fe"
      },
      "source": [
        "import re\n",
        "from konlpy.tag import Mecab\n",
        "tagger = Mecab()\n",
        "corpus = \"평생 살 것처럼 꿈을 꾸어라. 그리고 내일 죽을 것처럼 오늘을 살아라.\"\n",
        "tokens = tagger.morphs(re.sub(\"(\\.)\", \"\",corpus))\n",
        "\n",
        "vocab={}\n",
        "bow = []\n",
        "for tok in tokens :\n",
        "  if tok not in vocab.keys(): #bow가 없다면(처음엔 비어있음)\n",
        "    vocab[tok] = len(vocab) #vocab의 길이를 지정해준다.\n",
        "    bow.insert(len(vocab)-1, 1) #bow 리스트에 vocab길이에서 -1을 해서 넣어준다.)\n",
        "  else :\n",
        "    index = vocab.get(tok) # vocab의 index를 가져오고 \n",
        "    bow[index] = bow[index]+1 #해당하는 index에 값을 하나 증가시켜준다.(한번 더 나왔기 때문)\n",
        "print(bow)\n",
        "print(vocab)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1]\n",
            "{'평생': 0, '살': 1, '것': 2, '처럼': 3, '꿈': 4, '을': 5, '꾸': 6, '어라': 7, '그리고': 8, '내일': 9, '죽': 10, '오늘': 11, '아라': 12}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbS64Xr_W29z"
      },
      "source": [
        "21.09.28 \n",
        "1:43:30분까지 시청완료"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_QiwQSWVBSN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}