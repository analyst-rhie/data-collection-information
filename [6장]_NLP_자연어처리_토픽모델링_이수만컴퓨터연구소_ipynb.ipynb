{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[6장] NLP_자연어처리_토픽모델링_이수만컴퓨터연구소.ipynb.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPcrlP9/baTzu1Tl2kGdD3K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/analyst-rhie/data-collection-information/blob/main/%5B6%EC%9E%A5%5D_NLP_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%ED%86%A0%ED%94%BD%EB%AA%A8%EB%8D%B8%EB%A7%81_%EC%9D%B4%EC%88%98%EB%A7%8C%EC%BB%B4%ED%93%A8%ED%84%B0%EC%97%B0%EA%B5%AC%EC%86%8C_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2D-atLv-bUr"
      },
      "source": [
        "### 토픽 모델링(Topic Modeling)\n",
        "* 토픽 모델링은 문서 집합에서 주제를 찾아내기 위한 기술\n",
        "* 토픽 모델링은 '특정 주제에 관한 문서에서는 특정 단어가 자주 등장할 것이다'라는 직관을 기반\n",
        "* 예를 들어, 주제가 '개'인 문서에서는 개의 품종, 개의 특성을 나타내는 단어가 다른 문서에 비해 많이 등장\n",
        "* 주로 사용되는 토픽 모델링 방법은 잠재 의미 분석과 잠재 디리클레 할당 기법이 있음.\n",
        "  * 잠재 디리클레는 잠재 의미 분석의 발전 기법이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeXb8HrE-a57"
      },
      "source": [
        "### 잠재 의미 분석(Latent Semantic Analysis)\n",
        "* 잠재 의미 분석(LSA)은 주로 문서 색인의 의미 검석에 사용\n",
        "* 잠재 의미 인덱싱(Latent Semantic indexing, LSI)로도 알려져 있음.\n",
        "* LSA의 목표는 문서와 단어의 기반이 되는 잠재적인 토픽을 발견하는 것\n",
        "* 잠재적인 토픽은 문서에 있는 단어들의 분포를 주도한다고 가정\n",
        "* LSA 방법\n",
        "  * 문서 모음에서 생성한 문서-단어 행렬(Document Term Matrix)에서 단어-토픽 행렬(Term-Topic Matrix)과 토픽-중요도 행렬(Topic-Importance Matrix), 그리고 토픽-문서 행렬(Topic-Document Matrix)로 분해\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCDZret4_Uw2"
      },
      "source": [
        "### 잠재 디리클레 할당(Latent Dirichlet ALlocation)\n",
        "* 잠재 디리클레 할당은 대표적인 토픽 모델링 알고리즘 중 하나\n",
        "* 잠재 디리클레 할당 방법\n",
        "  1. 사용자가 토픽이 개수를 지정해 알고리즘에 전달\n",
        "  2. 모든 단어들을 토픽 중 하나에 할당\n",
        "  3. 모든 문서의 모든 단어에 단어 w가 가정에 의거 $p(t|d)$, $p(w|t)$에 따라 토픽을 재할당, 이를 반복, 이 때 가정은 자신만이 잘못된 토픽에 할당되어 있고 다른 모든 단어는 올바른 토픽에 할당된다는 것을 의미\n",
        "* $p(t|d)$ - 문서 d의 단어들 중 토픽 t에 해당하는 비율\n",
        "* 해당 문서의 자주 등장하는 다른 단어의 토픽이 해당 단어의 토픽이 될 가능성이 높음을 의미\n",
        "* $p(w|t)$ - 단어 w를 가지고 있는 모든 문서들 중 토픽 t가 할당된 비율\n",
        "* 다른 문서에서 단어 w에 많이 할당된 토픽이 해당 단어의 토픽이 될 가능성이 높음을 의미"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjxIWxX9Dx7r"
      },
      "source": [
        "### 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "pomRbSuv-V2P",
        "outputId": "41242d7d-ba5f-4b7a-e87d-0a1d4ecc183c"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# 토픽 모델링이 목적이므로 headers, footnotes, 인용문등은 삭제해준다.\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state = 1,\n",
        "                             remove = ('headers', 'footers', 'quotes')) \n",
        "documents = dataset.data\n",
        "\n",
        "print(len(documents))\n",
        "documents[0]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11314\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "Kiqea5mTEXUl",
        "outputId": "c10c2920-ec3c-47ce-d63f-cbe3c2cb1842"
      },
      "source": [
        "documents[3]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Notwithstanding all the legitimate fuss about this proposal, how much\\nof a change is it?  ATT\\'s last product in this area (a) was priced over\\n$1000, as I suspect \\'clipper\\' phones will be; (b) came to the customer \\nwith the key automatically preregistered with government authorities. Thus,\\naside from attempting to further legitimize and solidify the fed\\'s posture,\\nClipper seems to be \"more of the same\", rather than a new direction.\\n   Yes, technology will eventually drive the cost down and thereby promote\\nmore widespread use- but at present, the man on the street is not going\\nto purchase a $1000 crypto telephone, especially when the guy on the other\\nend probably doesn\\'t have one anyway.  Am I missing something?\\n   The real question is what the gov will do in a year or two when air-\\ntight voice privacy on a phone line is as close as your nearest pc.  That\\nhas got to a problematic scenario for them, even if the extent of usage\\nnever surpasses the \\'underground\\' stature of PGP.'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpNaN96PE2jA"
      },
      "source": [
        "* 텍스트 전처리가 필요함."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe0Y1JUlEzlk",
        "outputId": "48003827-b389-4d4d-f5fc-4d5876cf51a7"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(d):\n",
        "  pattern = r'[^a-zA-Z\\s]'\n",
        "  text = re.sub(pattern, '',d)\n",
        "  return text\n",
        "\n",
        "def clean_stopword(d):\n",
        "  stop_words = stopwords.words('english')\n",
        "  return ' '.join([w.lower() for w in d.split() if w.lower() not in stop_words and len(w) > 3])\n",
        "# 불용어 지우고 단어 3개 이하인거 제외하고 소문자로 바꾸고\n",
        "\n",
        "def preprocessing(d):\n",
        "  return preprocess_string(d)\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjc4D23QF6LK",
        "outputId": "4727a1be-9688-4c05-e6cc-55ebc97a0962"
      },
      "source": [
        "import pandas as pd\n",
        "# 전처리 전 전체 갯수 확인\n",
        "new_df = pd.DataFrame({'article':documents})\n",
        "len(new_df)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-bDU1UHGz4g",
        "outputId": "f748a6c5-48db-4242-85dd-dff09bafb804"
      },
      "source": [
        "new_df.replace(\"\",float(\"NaN\"),inplace=True)\n",
        "new_df.isnull().values.any() #null이 존재"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itX64oBQJbnN",
        "outputId": "4a801fcd-4973-4a4a-b68b-a733bfaac660"
      },
      "source": [
        "new_df.dropna(inplace= True) #null 삭제\n",
        "print(len(new_df))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "1MEvTzB4JttB",
        "outputId": "71321bf9-a660-4d55-aed7-f0ae84d37201"
      },
      "source": [
        "new_df['article'] = new_df['article'].apply(clean_text)\n",
        "new_df"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Well im not sure about the story nad it did se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\nYeah do you expect people to rea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Although I realize that principle is not one o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Well I will have to change the scoring on my p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11309</th>\n",
              "      <td>Danny Rubenstein an Israeli journalist will be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11310</th>\n",
              "      <td>\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11311</th>\n",
              "      <td>\\nI agree  Home runs off Clemens are always me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11312</th>\n",
              "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11313</th>\n",
              "      <td>\\nNo arg...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11096 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 article\n",
              "0      Well im not sure about the story nad it did se...\n",
              "1      \\n\\n\\n\\n\\n\\n\\nYeah do you expect people to rea...\n",
              "2      Although I realize that principle is not one o...\n",
              "3      Notwithstanding all the legitimate fuss about ...\n",
              "4      Well I will have to change the scoring on my p...\n",
              "...                                                  ...\n",
              "11309  Danny Rubenstein an Israeli journalist will be...\n",
              "11310                                                 \\n\n",
              "11311  \\nI agree  Home runs off Clemens are always me...\n",
              "11312  I used HP DeskJet with Orange Micros Grappler ...\n",
              "11313                                        \\nNo arg...\n",
              "\n",
              "[11096 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT1qjBV1KFgv",
        "outputId": "1a6a14f8-2a16-42a2-b9ef-0093a15f64f9"
      },
      "source": [
        "new_df['article'] = new_df['article'].apply(clean_stopword)\n",
        "new_df['article']"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        well sure story seem biased disagree statement...\n",
              "1        yeah expect people read actually accept hard a...\n",
              "2        although realize principle strongest points wo...\n",
              "3        notwithstanding legitimate fuss proposal much ...\n",
              "4        well change scoring playoff pool unfortunately...\n",
              "                               ...                        \n",
              "11309    danny rubenstein israeli journalist speaking t...\n",
              "11310                                                     \n",
              "11311    agree home runs clemens always memorable kinda...\n",
              "11312    used deskjet orange micros grappler system upd...\n",
              "11313    argument murphy scared hell came last year han...\n",
              "Name: article, Length: 11096, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C0xkU6KKUXW",
        "outputId": "ad9b944b-cc9d-4b9f-c27d-3e71298604ef"
      },
      "source": [
        "tokenized_news = new_df['article'].apply(preprocessing)\n",
        "tokenized_news = tokenized_news.to_list()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['yeah',\n",
              "  'expect',\n",
              "  'peopl',\n",
              "  'read',\n",
              "  'actual',\n",
              "  'accept',\n",
              "  'hard',\n",
              "  'atheism',\n",
              "  'need',\n",
              "  'littl',\n",
              "  'leap',\n",
              "  'faith',\n",
              "  'jimmi',\n",
              "  'logic',\n",
              "  'run',\n",
              "  'steam',\n",
              "  'sorri',\n",
              "  'piti',\n",
              "  'sorri',\n",
              "  'feel',\n",
              "  'denial',\n",
              "  'faith',\n",
              "  'need',\n",
              "  'pretend',\n",
              "  'happili',\n",
              "  'mayb',\n",
              "  'start',\n",
              "  'newsgroup',\n",
              "  'altatheisthard',\n",
              "  'wont',\n",
              "  'bummin',\n",
              "  'byeby',\n",
              "  'dont',\n",
              "  'forget',\n",
              "  'flintston',\n",
              "  'chewabl',\n",
              "  'bake',\n",
              "  'timmon'],\n",
              " ['realiz',\n",
              "  'principl',\n",
              "  'strongest',\n",
              "  'point',\n",
              "  'like',\n",
              "  'know',\n",
              "  'question',\n",
              "  'sort',\n",
              "  'arab',\n",
              "  'countri',\n",
              "  'want',\n",
              "  'continu',\n",
              "  'think',\n",
              "  'tank',\n",
              "  'charad',\n",
              "  'fixat',\n",
              "  'israel',\n",
              "  'stop',\n",
              "  'start',\n",
              "  'ask',\n",
              "  'sort',\n",
              "  'question',\n",
              "  'arab',\n",
              "  'countri',\n",
              "  'realiz',\n",
              "  'work',\n",
              "  'arab',\n",
              "  'countri',\n",
              "  'treatment',\n",
              "  'jew',\n",
              "  'decad',\n",
              "  'fixat',\n",
              "  'israel',\n",
              "  'begin',\n",
              "  'look',\n",
              "  'like',\n",
              "  'bias',\n",
              "  'attack',\n",
              "  'group',\n",
              "  'recogn',\n",
              "  'stupid',\n",
              "  'center',\n",
              "  'polici',\n",
              "  'research',\n",
              "  'fanci',\n",
              "  'bigot',\n",
              "  'hate',\n",
              "  'israel'],\n",
              " ['notwithstand',\n",
              "  'legitim',\n",
              "  'fuss',\n",
              "  'propos',\n",
              "  'chang',\n",
              "  'att',\n",
              "  'product',\n",
              "  'area',\n",
              "  'price',\n",
              "  'suspect',\n",
              "  'clipper',\n",
              "  'phone',\n",
              "  'came',\n",
              "  'custom',\n",
              "  'automat',\n",
              "  'preregist',\n",
              "  'govern',\n",
              "  'author',\n",
              "  'asid',\n",
              "  'attempt',\n",
              "  'legitim',\n",
              "  'solidifi',\n",
              "  'fed',\n",
              "  'postur',\n",
              "  'clipper',\n",
              "  'direct',\n",
              "  'technolog',\n",
              "  'eventu',\n",
              "  'drive',\n",
              "  'cost',\n",
              "  'promot',\n",
              "  'widespread',\n",
              "  'present',\n",
              "  'street',\n",
              "  'go',\n",
              "  'purchas',\n",
              "  'crypto',\n",
              "  'telephon',\n",
              "  'especi',\n",
              "  'probabl',\n",
              "  'doesnt',\n",
              "  'miss',\n",
              "  'real',\n",
              "  'question',\n",
              "  'year',\n",
              "  'tight',\n",
              "  'voic',\n",
              "  'privaci',\n",
              "  'phone',\n",
              "  'line',\n",
              "  'close',\n",
              "  'nearest',\n",
              "  'problemat',\n",
              "  'scenario',\n",
              "  'extent',\n",
              "  'usag',\n",
              "  'surpass',\n",
              "  'underground',\n",
              "  'statur'],\n",
              " ['chang',\n",
              "  'score',\n",
              "  'playoff',\n",
              "  'pool',\n",
              "  'unfortun',\n",
              "  'dont',\n",
              "  'time',\n",
              "  'right',\n",
              "  'certainli',\n",
              "  'post',\n",
              "  'score',\n",
              "  'rule',\n",
              "  'tomorrow',\n",
              "  'matter',\n",
              "  'youll',\n",
              "  'enter',\n",
              "  'good',\n",
              "  'keith',\n",
              "  'keller',\n",
              "  'let',\n",
              "  'ranger',\n",
              "  'let',\n",
              "  'quaker',\n",
              "  'kkellermailsasupennedu',\n",
              "  'leagu',\n",
              "  'champ'],\n",
              " ['read',\n",
              "  'think',\n",
              "  'morton',\n",
              "  'smith',\n",
              "  'jesu',\n",
              "  'magician',\n",
              "  'lazaru',\n",
              "  'wasnt',\n",
              "  'dead',\n",
              "  'go',\n",
              "  'tomb',\n",
              "  'initi',\n",
              "  'rite',\n",
              "  'magicult',\n",
              "  'jesu',\n",
              "  'appear',\n",
              "  'stai',\n",
              "  'normal',\n",
              "  'wonder'],\n",
              " ['record',\n",
              "  'show',\n",
              "  'iisi',\n",
              "  'cach',\n",
              "  'small',\n",
              "  'attach',\n",
              "  'measur',\n",
              "  'real',\n",
              "  'program',\n",
              "  'cach',\n",
              "  'speedup',\n",
              "  'vari',\n",
              "  'rang',\n",
              "  'think',\n",
              "  'averag',\n",
              "  'right',\n",
              "  'subject',\n",
              "  'differ',\n",
              "  'great',\n",
              "  'notic',\n",
              "  'simpl',\n",
              "  'cach',\n",
              "  'card',\n",
              "  'certainli',\n",
              "  'transform',\n",
              "  'iisi',\n",
              "  'enorm',\n",
              "  'better',\n",
              "  'convent',\n",
              "  'wisdom',\n",
              "  'sai',\n",
              "  'cach',\n",
              "  'card',\n",
              "  'maker',\n",
              "  'offer',\n",
              "  'speedup',\n",
              "  'differ',\n",
              "  'cach',\n",
              "  'bought',\n",
              "  'wave',\n",
              "  'absolut',\n",
              "  'problem',\n",
              "  'complet',\n",
              "  'speedomet',\n",
              "  'run',\n",
              "  'cach',\n",
              "  'like',\n",
              "  'let',\n",
              "  'check',\n",
              "  'convent',\n",
              "  'wisdom',\n",
              "  'call',\n",
              "  'perform',\n",
              "  'rate',\n",
              "  'number',\n",
              "  'cheer',\n",
              "  'file',\n",
              "  'convert',\n",
              "  'binhex',\n",
              "  'kbfkpqgeaqkabgjcfgigfhh',\n",
              "  'ghqqsqbucsquuqscfphghlhhclkjqtuldhhblucucjugqhgik',\n",
              "  'hcakrspijqrqqujnjucldussquutqcudljltuuquubut',\n",
              "  'sluuqjsutqjsudtuuqcqucfjcacsldld',\n",
              "  'jblaiggbjibpusbz',\n",
              "  'ikcqfmvxmhhairdjpcvrlucvszsijljpiflzqaalxrhjf',\n",
              "  'ljaztvllneapreivipiqkfdkzvbtlkcxzlghfik',\n",
              "  'alkhqqhhmilcqiqpgespejnbhmdhllqhlrecqbqsrmm',\n",
              "  'pakjpgxqcprryreydmhhrjhapclhrsgpfeiymcrisefp',\n",
              "  'cgevrtrafaqkapgjksfemcbikfqckuhrrbddtixpz',\n",
              "  'vldfeuvkujadfzaezregtyknxhsjfeigelvzj',\n",
              "  'vfbtbhjlexpekjzkhccfjkhgzcbhhnvdcxdsdfdpjei',\n",
              "  'qcxhkeldpyyyfrgjmrjtcymibkbbpubqpuqmmihgyrb',\n",
              "  'eungmsapmrikldinqmqjzabqmimqkcmcfeu',\n",
              "  'fyuexbtdraepadkcakeasthliahfhdabap',\n",
              "  'virtivsejgsqqqdpmmidhhmfruqekgevirqj',\n",
              "  'cmejfyiqlviqhhrhytrzkxipqrqkcuedrcr',\n",
              "  'qciukzmefiyibqmeffktbbjhylkpxvebuamdahccxl',\n",
              "  'iejfsappzchajcybrjdmfljbiscjhqetfvkrr',\n",
              "  'zviqrbthbfrhcpuyjujyprcdpaxafrlxrhfydmvi',\n",
              "  'dmtfnmlpjdxnkcrqbckehvkiarantabyyltm',\n",
              "  'fbufflgjdqareuqsjpbclefrkpdgjbijhnadhmrm',\n",
              "  'qpjiteramrhjajfhjrjjxdkecabdkjkrqricrcckrd',\n",
              "  'mvrbjcxnrfrchypkrchjjijhkjpqarbzffvkgqg',\n",
              "  'mbmupgxkuexglglglgekhrxfixirnykbmrsbnmqpqradj',\n",
              "  'fhrcpeirfhyjghpshhnrjjxbfbcljkfbqgmmcle',\n",
              "  'mhpfbarqqplkqiqrmmslfpcpjyddapaleeipbdc',\n",
              "  'rlpexjkhfzzkgnhabtlrdfaxvbkjmetlp',\n",
              "  'qckrpmjebnfjhnvpffjpqxjnlyrlpqccmlempcxzb',\n",
              "  'mpvspyxtbfqasrxrmrzfilxsvppvjicfmrrekxddifhmtqvlt',\n",
              "  'aalmchgurmeyaridiylpehpaifhnfmqqlchvcduajjebndih',\n",
              "  'pjqedxcgdtvmdypimrjerpqqaacvkclzizpzkfvrd',\n",
              "  'hfrmcinqhrejvernabfbqqlkclkqrhmkhnyaucqhq',\n",
              "  'kajhrefqrhbzslfgtpmifpcdembsc',\n",
              "  'crjjtbadmicjabap'],\n",
              " ['sound',\n",
              "  'like',\n",
              "  'wish',\n",
              "  'guess',\n",
              "  'socal',\n",
              "  'mean',\n",
              "  'peac',\n",
              "  'process',\n",
              "  'palestinean',\n",
              "  'negoci',\n",
              "  'wellfound',\n",
              "  'predict',\n",
              "  'know',\n",
              "  'found',\n",
              "  'test',\n",
              "  'tabl',\n",
              "  'month',\n",
              "  'prove',\n",
              "  'fault',\n",
              "  'right',\n",
              "  'know',\n",
              "  'let',\n",
              "  'palestinean',\n",
              "  'want',\n",
              "  'israel',\n",
              "  'known',\n",
              "  'accept',\n",
              "  'term',\n",
              "  'isra',\n",
              "  'mayb',\n",
              "  'palestinenan',\n",
              "  'readi',\n",
              "  'statehood',\n",
              "  'mayb',\n",
              "  'polit',\n",
              "  'palestinean',\n",
              "  'leadership',\n",
              "  'fraction',\n",
              "  'sai',\n",
              "  'reason',\n",
              "  'real',\n",
              "  'arab',\n",
              "  'stall',\n",
              "  'negoti',\n",
              "  'like',\n",
              "  'california',\n",
              "  'orang',\n",
              "  'feel',\n",
              "  'sharper',\n",
              "  'tabl',\n",
              "  'regard'],\n",
              " ['sai',\n",
              "  'shouldnt',\n",
              "  'allow',\n",
              "  'dont',\n",
              "  'forc',\n",
              "  'food',\n",
              "  'want',\n",
              "  'enhanc',\n",
              "  'tabl',\n",
              "  'like',\n",
              "  'salt',\n",
              "  'option',\n",
              "  'eater',\n",
              "  'commer',\n",
              "  'product',\n",
              "  'leav',\n",
              "  'includ',\n",
              "  'packet',\n",
              "  'like',\n",
              "  'salt',\n",
              "  'packet',\n",
              "  'desir',\n",
              "  'salt',\n",
              "  'pepper',\n",
              "  'mustard',\n",
              "  'ketchup',\n",
              "  'pickl',\n",
              "  'tabl',\n",
              "  'option',\n",
              "  'treat',\n",
              "  'wouldnt',\n",
              "  'shove',\n",
              "  'condiment',\n",
              "  'throat',\n",
              "  'dont',\n",
              "  'shove'],\n",
              " ['wonder',\n",
              "  'shed',\n",
              "  'light',\n",
              "  'electron',\n",
              "  'odomet',\n",
              "  'rememb',\n",
              "  'total',\n",
              "  'elaps',\n",
              "  'mileag',\n",
              "  'kind',\n",
              "  'memori',\n",
              "  'stablereli',\n",
              "  'nonvolatil',\n",
              "  'independ',\n",
              "  'outsid',\n",
              "  'batteri',\n",
              "  'power',\n",
              "  'year',\n",
              "  'life',\n",
              "  'vehicl',\n",
              "  'amaz',\n",
              "  'like',\n",
              "  'expect',\n",
              "  'work',\n",
              "  'length',\n",
              "  'time',\n",
              "  'especi',\n",
              "  'light',\n",
              "  'gizmo',\n",
              "  'work',\n",
              "  'good',\n",
              "  'work',\n",
              "  'month',\n",
              "  'break',\n",
              "  'question',\n",
              "  'legal',\n",
              "  'ramif',\n",
              "  'sell',\n",
              "  'replac',\n",
              "  'odomet',\n",
              "  'start',\n",
              "  'mile',\n",
              "  'actual',\n",
              "  'mile',\n",
              "  'look',\n",
              "  'like',\n",
              "  'fraud',\n",
              "  'fairli',\n",
              "  'easi',\n",
              "  'price',\n",
              "  'odomet',\n",
              "  'mile',\n",
              "  'want',\n",
              "  'tell',\n",
              "  'buyer',\n",
              "  'thank',\n",
              "  'insight']]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QepnpJqVNgvW",
        "outputId": "e57e5fff-fa80-457c-cbda-361a8a972c1a"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "drop_news = [index for index, sentence in enumerate(tokenized_news) if len(sentence) <=1]\n",
        "# sentence가 1개 이하인 것들 제거\n",
        "\n",
        "news_texts = np.delete(tokenized_news, drop_news, axis= 0)\n",
        "print(len(news_texts))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqfqw-ONOfUz"
      },
      "source": [
        "### Genism을 이용한 토픽 모델링"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHQSsb4nOiFp",
        "outputId": "f6886dc8-da7a-4e36-e70b-7ed74edea613"
      },
      "source": [
        "from gensim import corpora\n",
        "\n",
        "dictionary = corpora.Dictionary(news_texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in news_texts] #문서를 bow로 바꿈\n",
        "print(corpus[1])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 2), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 2), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 2), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVBdozjxO50U"
      },
      "source": [
        "* 토픽모델링 할때 딕셔너리랑 코퍼스 값들이 필요하다. 그래서 준비를 해둔 것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQdFTVcoPE_3"
      },
      "source": [
        "### 잠재 의미 분석을 위한 LsiModel\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpsDiARNODod",
        "outputId": "b1182c78-546f-4e30-e87e-a0c126c0d96e"
      },
      "source": [
        "from gensim.models import LsiModel\n",
        "#lsi model은 LDA 보다 빠르다는 장점이 있다.\n",
        "lsi_model = LsiModel(corpus, num_topics = 20, id2word = dictionary) # 뉴스 그룹이 예시가 20개 였다.\n",
        "topics = lsi_model.print_topics()\n",
        "topics"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '1.000*\"maxaxaxaxaxaxaxaxaxaxaxaxaxaxax\" + 0.008*\"mgvgvgvgvgvgvgvgvgvgvgvgvgvgvgv\" + 0.005*\"maxaxaxaxaxaxaxaxaxaxaxaxaxax\" + 0.003*\"maxaxaxaxaxaxaxaxaxaxaxaxaxaxaxq\" + 0.002*\"maxaxaxaxaxaxaxaxaxaxaxaxaxaxf\" + 0.002*\"mqaxaxaxaxaxaxaxaxaxaxaxaxaxax\" + 0.001*\"maxaxaxaxaxaxaxaxaxaxaxaxaxasqq\" + 0.001*\"maxaxaxaxaxaxaxaxaxaxaxaxaxaxqq\" + 0.001*\"maxaxaxaxaxaxaxaxaxaxaxaxaxaxasq\" + 0.001*\"maxaxaxaxaxaxaxaxaxaxaxaxaxaxqqf\"'),\n",
              " (1,\n",
              "  '0.393*\"file\" + 0.191*\"program\" + 0.158*\"imag\" + 0.126*\"peopl\" + 0.125*\"avail\" + 0.119*\"inform\" + 0.116*\"includ\" + 0.116*\"entri\" + 0.114*\"work\" + 0.112*\"dont\"'),\n",
              " (2,\n",
              "  '0.456*\"file\" + -0.215*\"peopl\" + -0.210*\"know\" + -0.192*\"said\" + -0.176*\"dont\" + 0.158*\"entri\" + -0.158*\"think\" + -0.153*\"stephanopoulo\" + 0.139*\"imag\" + -0.129*\"go\"'),\n",
              " (3,\n",
              "  '0.409*\"file\" + 0.286*\"entri\" + -0.241*\"imag\" + -0.168*\"avail\" + -0.141*\"wire\" + -0.136*\"data\" + -0.122*\"version\" + 0.116*\"onam\" + -0.109*\"window\" + 0.104*\"said\"'),\n",
              " (4,\n",
              "  '-0.618*\"wire\" + -0.250*\"ground\" + -0.188*\"circuit\" + -0.180*\"outlet\" + 0.163*\"imag\" + -0.149*\"connect\" + -0.141*\"neutral\" + -0.139*\"entri\" + -0.134*\"gfci\" + -0.112*\"cabl\"'),\n",
              " (5,\n",
              "  '-0.370*\"imag\" + -0.344*\"jpeg\" + 0.339*\"anonym\" + 0.186*\"post\" + 0.185*\"internet\" + 0.164*\"privaci\" + 0.138*\"inform\" + 0.126*\"email\" + -0.126*\"format\" + 0.125*\"mail\"'),\n",
              " (6,\n",
              "  '0.405*\"entri\" + -0.393*\"file\" + 0.235*\"program\" + -0.191*\"jpeg\" + -0.130*\"anonym\" + 0.127*\"widget\" + -0.125*\"imag\" + -0.120*\"firearm\" + -0.117*\"wire\" + 0.113*\"drive\"'),\n",
              " (7,\n",
              "  '0.341*\"drive\" + 0.243*\"stephanopoulo\" + -0.227*\"anonym\" + 0.196*\"control\" + 0.193*\"presid\" + 0.190*\"disk\" + -0.158*\"peopl\" + -0.147*\"jpeg\" + -0.144*\"post\" + 0.143*\"support\"'),\n",
              " (8,\n",
              "  '0.427*\"stephanopoulo\" + -0.315*\"drive\" + 0.284*\"presid\" + -0.171*\"disk\" + -0.162*\"control\" + -0.160*\"armenian\" + -0.141*\"peopl\" + 0.120*\"imag\" + -0.111*\"didnt\" + -0.110*\"hard\"'),\n",
              " (9,\n",
              "  '-0.272*\"team\" + -0.248*\"hockei\" + -0.220*\"game\" + -0.219*\"leagu\" + 0.181*\"anonym\" + 0.170*\"stephanopoulo\" + -0.163*\"season\" + -0.153*\"year\" + 0.151*\"drive\" + -0.138*\"imag\"'),\n",
              " (10,\n",
              "  '0.328*\"widget\" + -0.323*\"drive\" + -0.218*\"imag\" + -0.208*\"jpeg\" + -0.208*\"entri\" + 0.181*\"window\" + -0.177*\"disk\" + 0.176*\"applic\" + 0.158*\"resourc\" + -0.157*\"anonym\"'),\n",
              " (11,\n",
              "  '0.274*\"launch\" + 0.252*\"space\" + -0.215*\"hockei\" + -0.206*\"team\" + -0.198*\"game\" + -0.186*\"leagu\" + 0.180*\"satellit\" + -0.178*\"stephanopoulo\" + -0.132*\"season\" + -0.118*\"drive\"'),\n",
              " (12,\n",
              "  '0.206*\"jesu\" + -0.184*\"launch\" + 0.179*\"atheist\" + 0.163*\"christian\" + -0.160*\"space\" + 0.152*\"jpeg\" + 0.152*\"believ\" + 0.136*\"exist\" + -0.124*\"satellit\" + -0.122*\"didnt\"'),\n",
              " (13,\n",
              "  '0.317*\"launch\" + 0.273*\"space\" + 0.202*\"satellit\" + -0.149*\"govern\" + -0.143*\"administr\" + 0.141*\"stephanopoulo\" + -0.138*\"russian\" + -0.132*\"program\" + -0.132*\"armenian\" + 0.125*\"post\"'),\n",
              " (14,\n",
              "  '0.242*\"jpeg\" + -0.175*\"stephanopoulo\" + 0.152*\"russian\" + 0.150*\"administr\" + -0.137*\"entri\" + 0.133*\"russia\" + -0.128*\"jesu\" + 0.124*\"govern\" + -0.122*\"imag\" + 0.121*\"work\"'),\n",
              " (15,\n",
              "  '-0.390*\"entri\" + 0.270*\"onam\" + 0.251*\"output\" + 0.218*\"char\" + 0.167*\"stream\" + 0.143*\"eofnotok\" + 0.135*\"imag\" + 0.124*\"line\" + 0.120*\"write\" + -0.118*\"jpeg\"'),\n",
              " (16,\n",
              "  '0.269*\"south\" + 0.232*\"secret\" + 0.219*\"island\" + 0.216*\"rockefel\" + 0.174*\"militari\" + 0.171*\"nuclear\" + 0.168*\"ship\" + 0.164*\"bolshevik\" + 0.150*\"stephanopoulo\" + 0.148*\"georgia\"'),\n",
              " (17,\n",
              "  '0.197*\"widget\" + -0.163*\"jesu\" + 0.162*\"convert\" + 0.157*\"health\" + 0.146*\"applic\" + 0.144*\"stephanopoulo\" + -0.136*\"version\" + 0.131*\"data\" + 0.131*\"valu\" + -0.126*\"avail\"'),\n",
              " (18,\n",
              "  '-0.191*\"window\" + 0.188*\"turkish\" + -0.185*\"health\" + 0.160*\"jew\" + 0.156*\"imag\" + -0.142*\"jpeg\" + 0.141*\"data\" + 0.137*\"entri\" + 0.129*\"widget\" + -0.124*\"report\"'),\n",
              " (19,\n",
              "  '-0.410*\"turkish\" + -0.359*\"jew\" + -0.217*\"armenian\" + -0.216*\"turkei\" + -0.200*\"stephanopoulo\" + -0.188*\"jpeg\" + 0.162*\"data\" + 0.144*\"imag\" + -0.143*\"nazi\" + -0.116*\"book\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owo_D1NhPz2z"
      },
      "source": [
        "* 사실 토픽개수가 몇개인지 판단하기 힘듦.\n",
        "\n",
        "21.10.5 22분까지 완"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnwVRdy-R24Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMa-WCD4Rp__"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tII5piD2PgPO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}